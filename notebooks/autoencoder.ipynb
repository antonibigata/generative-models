{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from omegaconf import OmegaConf\n",
    "from sgm.util import instantiate_from_config\n",
    "\n",
    "config = OmegaConf.load(\n",
    "    \"/vol/paramonos2/projects/antoni/code/Personal/generative-models/configs/example_training/svd_interpolation.yaml\"\n",
    ")\n",
    "print(config)\n",
    "config[\"model\"][\"params\"][\"ckpt_path\"] = \"../checkpoints/svd.safetensors\"\n",
    "video_model = instantiate_from_config(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.learned_mask.sum((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_video\n",
    "import torch\n",
    "\n",
    "video_path = \"/data2/Datasets/HDTF/cropped_videos_original/WRA_BobbySchilling_001.mp4\"\n",
    "resolution = 512\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "video, audio, info = read_video(video_path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
    "\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "resize = Resize((resolution, resolution))\n",
    "video = resize(video)\n",
    "print(video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"/data2/Datasets/HDTF/audio/WRA_BobbySchilling_001_whisper_emb.pt\"\n",
    "audio = torch.load(audio_path)\n",
    "print(audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video.min(), video.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.first_stage_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "encoded = model.conditioner.embedders[3]((video[:2].to(device).float() / 255.0) * 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "video_model.to(device)\n",
    "# decoded = video_model.decode_first_stage(encoded[:, 4:])\n",
    "decoded = video_model.decode_first_stage(encoded)\n",
    "img = Image.fromarray(np.uint8((decoded[0].clip(-1, 1).cpu().numpy().transpose(1, 2, 0) + 1) * 255 / 2))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "latent_path = \"/vol/bitbucket/abigata/00190_video_512_latent.safetensors\"\n",
    "\n",
    "latent = load_file(latent_path)\n",
    "latent[\"latents\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = video_model.decode_first_stage(latent[\"latents\"][:1].to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.fromarray(np.uint8((decoded[0].clip(-1, 1).cpu().numpy().transpose(1, 2, 0) + 1) * 255 / 2))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from omegaconf import OmegaConf\n",
    "from sgm.util import instantiate_from_config\n",
    "\n",
    "config = OmegaConf.load(\n",
    "    \"/vol/paramonos2/projects/antoni/code/Personal/generative-models/configs/inference/sd_2_1.yaml\"\n",
    ")\n",
    "print(config)\n",
    "config[\"model\"][\"params\"][\"ckpt_path\"] = (\n",
    "    \"/vol/paramonos2/projects/antoni/code/Personal/generative-models/checkpoints/v2-1_512-ema-pruned.safetensors\"\n",
    ")\n",
    "model = instantiate_from_config(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_video\n",
    "import torch\n",
    "\n",
    "video_path = \"/data2/Datasets/HDTF/cropped_videos_original/WRA_BobbySchilling_001.mp4\"\n",
    "resolution = 512\n",
    "device = torch.device(\"cuda:4\")\n",
    "\n",
    "video, audio, info = read_video(video_path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
    "\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "resize = Resize((resolution, resolution))\n",
    "video = resize(video)\n",
    "print(video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "encoded = model.encode_first_stage((video[:2].to(device).float() / 255.0) * 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded = model.decode_first_stage(encoded)\n",
    "decoded = model.decode_first_stage(latent[\"latents\"][:1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.fromarray(np.uint8((decoded[0].clip(-1, 1).cpu().numpy().transpose(1, 2, 0) + 1) * 255 / 2))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "# load model and tokenizer\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\", cache_dir=\"/vol/bitbucket/abigata/.cache\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", cache_dir=\"/vol/bitbucket/abigata/.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "audio_path = \"/data2/Datasets/HDTF/audio/WRA_JeffFlake_001.wav\"\n",
    "audio, sr = torchaudio.load(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "audio_reshaped = rearrange(audio.mean(0)[:640000], \"(f s) -> f s\", s=320)\n",
    "audio_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = processor(audio=audio_reshaped, sampling_rate=sr, return_tensors=\"pt\", padding=\"longest\").input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "normed_slice = (audio - audio.mean()) / torch.sqrt(audio.var() + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outs = model(normed_slice[:, : 16000 * 3], output_hidden_states=True)\n",
    "    # hidden_states = model.wav2vec2(normed_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "rearanged = rearrange(hidden_states[0], \"() (f d) c -> f d c\", d=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearanged[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs.hidden_states[-1][:, 2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_video\n",
    "\n",
    "video_path = \"/data2/Datasets/HDTF/cropped_videos_original/WRA_JeffFlake_001.mp4\"\n",
    "resolution = 512\n",
    "device = torch.device(\"cuda:4\")\n",
    "\n",
    "video, audio, info = read_video(video_path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs.logits.shape, 7500 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = torch.argmax(outs.logits, dim=-1).squeeze()\n",
    "\n",
    "# transcribe\n",
    "transcription = processor.decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"NTRIES HAVE NEVER HAD FOR A SINGLE DAY IN THEIR LIVES BUT THE STORY OF THE PAST THREE AND A HALF YEARS IS A STORY OF THE POWER THAT WE VEST IN THE PRESIDENCY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "# load model and tokenizer\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\", cache_dir=\"/vol/bitbucket/abigata/.cache\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", cache_dir=\"/vol/bitbucket/abigata/.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "audio_emb = torch.load(\"/data2/Datasets/VoxCeleb2/video/dev/mp4/id07085/NcpRWNyzlAA/00031_wav2vec2_emb.pt\")\n",
    "audio_emb = rearrange(audio_emb, \"f d c -> (f d) c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.lm_head(audio_emb).unsqueeze(0)\n",
    "predicted_ids = torch.argmax(logits, dim=-1).squeeze()\n",
    "\n",
    "# transcribe\n",
    "transcription = processor.decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot video\n",
    "import moviepy.editor as mp\n",
    "\n",
    "mp.VideoFileClip(\"/vol/paramonos/datasets/VoxCeleb2/video/dev/mp4/id07085/NcpRWNyzlAA/00031.mp4\").ipython_display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
