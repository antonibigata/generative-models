{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "model_ckpt = load_file(\"../checkpoints/svd.safetensors\")\n",
    "\n",
    "print(model_ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/antoni/miniconda3/envs/svd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([0, 1, 1, 3]), {}, 0.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.io import read_video\n",
    "\n",
    "video, audio, info = read_video(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/AA_processed/part_8/video_crop/D550_C047_0106XS_001_output_output.mp4\"\n",
    ")\n",
    "video.shape, info, video.shape[0] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1912491]), 16000, 119.5306875)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "audio, sr = torchaudio.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/AA_processed/part_8/audio/D550_C047_0106XS_001.wav\"\n",
    ")\n",
    "audio.shape, sr, audio.shape[-1] / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\n",
    "    torch.load(\n",
    "        \"/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/audio_emb/1/-6yEyJRIUGc_5_0_0_170_beats_emb.pt\"\n",
    "    ).shape\n",
    ")\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "torch.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/audio_emb/1/-6yEyJRIUGc_5_0_0_170_beats_emb.pt\"\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/audio_emb/1/-6yEyJRIUGc_5_0_0_170_wav2vec2_emb.safetensors\"\n",
    ")[\"audio\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def max_divisions(a, b):\n",
    "    if b == 1:\n",
    "        return float(\"inf\")  # Dividing by 1 will never reduce the number\n",
    "    return math.floor(math.log(a, b))\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "a = 100\n",
    "b = 14\n",
    "print(max_divisions(a, b))  # Output: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\n",
    "    \"../logs/2024-08-21T12-15-52_example_training-svd_interpolation_no_emb/checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ckpt[\"module\"].keys():\n",
    "    if \"learned_mask\" in k:\n",
    "        print(k)\n",
    "        print(ckpt[\"module\"][k])\n",
    "        print(model_ckpt[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(\n",
    "    \"../logs/2024-05-30T18-28-54_example_training-svd_image/checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict[\"module\"].keys()\n",
    "# Take only weights that contain the word \"diffusion_model\" and keep only what comes after the word \"diffusion_model\" but including the word \"diffusion_model\"\n",
    "new_dict = {}\n",
    "for k, v in state_dict[\"module\"].items():\n",
    "    if \"diffusion_model\" in k:\n",
    "        new_dict[\"diffusion_model\" + k.split(\"diffusion_model\")[1]] = v\n",
    "new_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpth_path = \"/vol/bitbucket/abigata/checkpoints/trainstep_checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\"\n",
    "ckpt = torch.load(ckpth_path)\n",
    "\n",
    "print(ckpt[\"state_dict\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new weights for modified model\n",
    "new_weights = {}\n",
    "for k, v in model_ckpt.items():\n",
    "    # Remove model weights\n",
    "    # if k.startswith(\"model.\"):\n",
    "    #     continue\n",
    "    # Since embedder 1 and 2 have been removed, we need to remove the corresponding weights and change number of following embedders\n",
    "    if \"embedders.1\" in k or \"embedders.2\" in k:\n",
    "        print(\"chybrax\")\n",
    "        continue\n",
    "    if \"embedders.3\" in k:\n",
    "        new_weights[k.replace(\"embedders.3\", \"embedders.1\")] = v\n",
    "        continue\n",
    "    new_weights[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new weights\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(new_weights, \"../checkpoints/svd_no_emb.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Function to ensure all tensors are [1, 640]\n",
    "def ensure_shape(tensors, target_length=640):\n",
    "    processed_tensors = []\n",
    "    for tensor in tensors:\n",
    "        current_length = tensor.shape[1]\n",
    "        diff = current_length - target_length\n",
    "        assert abs(diff) <= 5, f\"Expected shape {target_length}, but got {current_length}\"\n",
    "        if diff < 0:\n",
    "            # Calculate how much padding is needed\n",
    "            padding_needed = target_length - current_length\n",
    "            # Pad the tensor\n",
    "            padded_tensor = F.pad(tensor, (0, padding_needed))\n",
    "            processed_tensors.append(padded_tensor)\n",
    "        elif diff > 0:\n",
    "            # Trim the tensor\n",
    "            trimmed_tensor = tensor[:, :target_length]\n",
    "            processed_tensors.append(trimmed_tensor)\n",
    "        else:\n",
    "            # If it's already the correct size\n",
    "            processed_tensors.append(tensor)\n",
    "    return torch.cat(processed_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decord import AVReader\n",
    "from decord import cpu, gpu\n",
    "import decord\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "import torch\n",
    "\n",
    "av = AVReader(\n",
    "    \"/vol/paramonos2/projects/antoni/code/Personal/generative-models/outputs/simple_video_sample/svd/000096_gt.mp4\",\n",
    "    ctx=cpu(0),\n",
    "    sample_rate=16000,\n",
    ")\n",
    "# To access both the video frames and corresponding audio samples\n",
    "audio, video = av[0:8]\n",
    "# for a in audio:\n",
    "#     print(a.shape)\n",
    "# audio = torch.cat(audio, dim=0)\n",
    "# # Each element in audio will be a batch of samples corresponding to a frame of video\n",
    "# print('Frame #: ', len(audio), len(video))\n",
    "# print('Shape of the audio samples of the first frame: ', audio.shape, len(audio))\n",
    "# print('Shape of the first frame: ', video[0].shape)\n",
    "# Similarly, to get a batch\n",
    "audio2, video2 = av.get_batch(range(0, len(av)))\n",
    "audio2 = ensure_shape(audio2)\n",
    "print(\"Shape of the audio samples of the first frame: \", audio2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av._AVReader__video_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "raw_audio = rearrange(audio2, \"t c -> (t c)\")\n",
    "raw_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "\n",
    "write(\"test.wav\", 16000, raw_audio.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot video\n",
    "import moviepy.editor as mp\n",
    "\n",
    "mp.VideoFileClip(\n",
    "    \"/vol/paramonos2/projects/antoni/code/Personal/generative-models/outputs/simple_video_sample/svd/000059.mp4\"\n",
    ").ipython_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import glob\n",
    "\n",
    "tar_files = glob.glob(\"/data/122-2/Datasets/CREMA/webdataset/train/batch_*.tar\")\n",
    "\n",
    "\n",
    "def preprocess_video(video):\n",
    "    # Your video preprocessing code here\n",
    "    return video\n",
    "\n",
    "\n",
    "def preprocess_audio(audio):\n",
    "    # Your audio preprocessing code here\n",
    "    return audio\n",
    "\n",
    "\n",
    "# Define how each type of file should be processed\n",
    "def process_sample(sample):\n",
    "    # Assuming the keys are 'video.mp4' and 'audio.wav' in your tar files\n",
    "    video, audio = sample[\"mp4\"], sample[\"wav\"]\n",
    "\n",
    "    # Apply your preprocessing functions\n",
    "    processed_video = preprocess_video(video)\n",
    "    processed_audio = preprocess_audio(audio)\n",
    "\n",
    "    return processed_video, processed_audio\n",
    "\n",
    "\n",
    "def custom_decoder(sample):\n",
    "    # Add custom decoding or handling for specific file types\n",
    "    # For example, if sample contains a '.wav' key, decode it appropriately\n",
    "    print(sample.keys())\n",
    "    return sample\n",
    "\n",
    "\n",
    "ds = wds.WebDataset(tar_files).map(custom_decoder)\n",
    "for sample in ds:\n",
    "    print(sample.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ffmpeg\n",
    "import tempfile\n",
    "\n",
    "\n",
    "class ResolutionSubsampler:\n",
    "    \"\"\"\n",
    "    Adjusts the resolution of the videos to the specified height and width.\n",
    "\n",
    "    Args:\n",
    "        video_size (int): Target resolution of the videos.\n",
    "        resize_mode (list[str]): List of resize modes to apply. Possible options are:\n",
    "            scale: scale video keeping aspect ratios (currently always picks video height)\n",
    "            crop: center crop to video_size x video_size\n",
    "            pad: center pad to video_size x video_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_size, resize_mode):\n",
    "        self.video_size = video_size\n",
    "        self.resize_mode = resize_mode\n",
    "\n",
    "    def __call__(self, streams):\n",
    "        video_bytes = streams[\"video\"]\n",
    "        subsampled_bytes = []\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            with open(os.path.join(tmpdir, \"input.mp4\"), \"wb\") as f:\n",
    "                f.write(video_bytes)\n",
    "            try:\n",
    "                _ = ffmpeg.input(f\"{tmpdir}/input.mp4\")\n",
    "                if \"scale\" in self.resize_mode:\n",
    "                    _ = _.filter(\"scale\", -2, self.video_size)\n",
    "                if \"crop\" in self.resize_mode:\n",
    "                    _ = _.filter(\"crop\", w=self.video_size, h=self.video_size)\n",
    "                if \"pad\" in self.resize_mode:\n",
    "                    _ = _.filter(\"pad\", w=self.video_size, h=self.video_size)\n",
    "                _ = _.output(f\"{tmpdir}/output.mp4\", reset_timestamps=1).run(capture_stdout=True, quiet=True)\n",
    "            except Exception as err:  # pylint: disable=broad-except\n",
    "                return [], None, str(err)\n",
    "\n",
    "            with open(f\"{tmpdir}/output.mp4\", \"rb\") as f:\n",
    "                subsampled_bytes = f.read()\n",
    "        streams[\"video\"] = subsampled_bytes\n",
    "        return streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_decoder(sample):\n",
    "    # Add custom decoding or handling for specific file types\n",
    "    # For example, if sample contains a '.wav' key, decode it appropriately\n",
    "    for key in sample.keys():\n",
    "        if key.endswith(\"video\"):\n",
    "            sample[key] = wds.torch_video(\"mp4\", sample[key])\n",
    "        if key.endswith(\"audio\"):\n",
    "            sample[key] = wds.torch_audio(\"wav\", sample[key])\n",
    "        elif key.endswith(\"audio_emb\"):\n",
    "            sample[key] = wds.torch_loads(sample[key])\n",
    "    return sample\n",
    "\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "i = 0\n",
    "ds = (\n",
    "    wds.WebDataset(\"/data/122-2/Datasets/CREMA/webdataset/train/out-{000000..000006}.tar\")\n",
    "    .rename(video=\"mp4;video\", audio=\"wav;audio\", audio_emb=\"pt;audio_emb\")\n",
    "    .map(ResolutionSubsampler(256, [\"scale\"]))\n",
    "    .map(custom_decoder)\n",
    ")\n",
    "for sample in ds:\n",
    "    print(sample.keys())\n",
    "    i += 1\n",
    "    # if i > 10:\n",
    "    break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "\n",
    "i = 0\n",
    "ds = wds.WebDataset(\"/data/122-2/Datasets/CREMA/webdataset/train/out-{000000..000006}.tar\").decode(\"torchl\")\n",
    "for sample in ds:\n",
    "    print(sample.keys())\n",
    "    # print(sample['mp4'][0].shape)\n",
    "    print(sample[\"pt\"].shape)\n",
    "    i += 1\n",
    "    # if i > 10:\n",
    "    break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/122-2/Datasets/CREMA/s32/audio/1032_IEO_FEA_LO_emb.pt\", \"rb\") as f:\n",
    "    video = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "\n",
    "wds.autodecode.decoders\n",
    "wds.autodecode.imagespecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdata import create_dataset, create_loader\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = OmegaConf.load(\"/vol/paramonos2/projects/antoni/code/Personal/generative-models/notebooks/data.yaml\")\n",
    "\n",
    "# build config\n",
    "datapipeline = create_dataset(**config.dataset)\n",
    "\n",
    "i = 0\n",
    "for sample in tqdm(datapipeline, desc=\"Loading dataset\"):\n",
    "    i += 1\n",
    "    break\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapipeline.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cond_frames\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cond = ((sample[\"cond_frames\"].permute(1, 2, 0).numpy() + 1) / 2).clip(0, 1) * 255\n",
    "cond = cond.astype(np.uint8)\n",
    "plt.imshow(cond)\n",
    "print(sample[\"cond_aug\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum\n",
    "\n",
    "scale = 1\n",
    "num_audio_frames = 28\n",
    "q = torch.randn(1, 14, 320)\n",
    "k = torch.randn(1, num_audio_frames, 320)\n",
    "v = torch.randn(1, num_audio_frames, 320)\n",
    "\n",
    "sim = einsum(\"b i d, b j d -> b i j\", q, k) * scale\n",
    "# del q, k\n",
    "\n",
    "# attention, what we cannot get enough of\n",
    "sim = sim.softmax(dim=-1)\n",
    "\n",
    "out = einsum(\"b i j, b j d -> b i d\", sim, v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = \"/vol/bitbucket/abigata/mp_rank_00_model_states.pt\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Attempt to load the .pt file\n",
    "    data = torch.load(checkpoint_path)\n",
    "    print(\"File loaded successfully. Integrity check passed.\")\n",
    "except Exception as e:\n",
    "    # Handle exceptions that indicate file loading issues\n",
    "    print(f\"Failed to load file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "sd = {k.replace(\"_forward_module.\", \"\"): v for k, v in checkpoint[\"module\"].items()}\n",
    "save_file(sd, checkpoint_path.replace(\".pt\", \".safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in checkpoint[\"module\"]:\n",
    "    if \"model.diffusion_model.input_blocks.1.0.time_stack.emb_layers.1.weight\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def select_indices(max_frames, n):\n",
    "    # Step 1: Randomly select the first index\n",
    "    index1 = random.randint(0, max_frames)\n",
    "\n",
    "    # Step 2: Determine the valid range for the second index\n",
    "    valid_range = list(range(0, index1 - n)) + list(range(index1 + n + 1, max_frames + 1))\n",
    "\n",
    "    # Check if the valid range is not empty\n",
    "    if not valid_range:\n",
    "        raise ValueError(\"The range is too small for these constraints!\")\n",
    "\n",
    "    # Step 3: Randomly select the second index from the valid range\n",
    "    index2 = random.choice(valid_range)\n",
    "\n",
    "    return index1, index2\n",
    "\n",
    "\n",
    "# Usage example\n",
    "max_frames = 100  # for example, 100 frames\n",
    "n = 10  # must be at least 10 indices apart\n",
    "indices = select_indices(max_frames, n)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/fsx/rs2517/data/HDTF/cropped_videos_original/WRA_ToddYoung_000_video_512_latent.safetensors\"\n",
    "tensors = {}\n",
    "with safe_open(path, framework=\"pt\") as f:\n",
    "    tensor_slice = f.get_slice(\"latents\")\n",
    "    print(tensor_slice.get_shape())\n",
    "    tensor = tensor_slice[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 25):\n",
    "    for j in range(i, 25):\n",
    "        if tensor[6250 + i].isclose(tensor[6250 + j]).all():\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_indexes(main_index, n_audio_frames, max_len):\n",
    "    # Get indexes for audio from both sides of the main index\n",
    "    audio_ids = []\n",
    "    # get audio embs from both sides of the GT frame\n",
    "    audio_ids += [0] * max(n_audio_frames - main_index, 0)\n",
    "    for i in range(max(main_index - n_audio_frames, 0), min(main_index + n_audio_frames + 1, max_len)):\n",
    "        # for i in range(frame_ids[0], min(frame_ids[0] + self.n_audio_motion_embs + 1, n_frames)):\n",
    "        audio_ids += [i]\n",
    "    audio_ids += [max_len - 1] * max(main_index + n_audio_frames - max_len + 1, 0)\n",
    "    return (audio_ids,)\n",
    "\n",
    "\n",
    "get_audio_indexes(27, 2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/fsx/rs2517/data/HDTF/audio/WRA_ToddYoung_000_wav2vec2_emb.safetensors\"\n",
    "tensors = {}\n",
    "with safe_open(path, framework=\"pt\") as f:\n",
    "    tensor_slice = f.get_slice(\"audio\")\n",
    "    print(tensor_slice.get_shape())\n",
    "tensor_slice[[0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "vr = decord.VideoReader(\"/fsx/rs2517/data/HDTF/cropped_videos_original/WRA_ToddYoung_000.mp4\")\n",
    "print(len(vr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "valid_files = 0\n",
    "files = glob.glob(\"/fsx/rs2517/data/HDTF/cropped_videos_original/*.safetensors\")\n",
    "for file in files:\n",
    "    with safe_open(file, framework=\"pt\") as f:\n",
    "        tensor_slice = f.get_slice(\"latents\")\n",
    "        latent_shape = tensor_slice.get_shape()[0]\n",
    "\n",
    "    file = file.replace(\"cropped_videos_original\", \"audio\").replace(\"video_512_latent\", \"wav2vec2_emb\")\n",
    "    with safe_open(file, framework=\"pt\") as f:\n",
    "        tensor_slice = f.get_slice(\"audio\")\n",
    "        audio_shape = tensor_slice.get_shape()[0]\n",
    "\n",
    "    if latent_shape != audio_shape:\n",
    "        print(\n",
    "            f\"latent_shape: {latent_shape}, audio_shape: {audio_shape}. Difference: {abs(latent_shape - audio_shape)}\"\n",
    "        )\n",
    "    else:\n",
    "        valid_files += 1\n",
    "print(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "scale = torch.linspace(5, 2, 14).unsqueeze(0)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_image_preds = [i for i in range(0, 15 * 25, 13)]\n",
    "print(audio_image_preds)\n",
    "len_audio_image_preds = len(audio_image_preds)\n",
    "print(len_audio_image_preds)\n",
    "for i in range(14, len_audio_image_preds + len_audio_image_preds % 14, 14):\n",
    "    print(i)\n",
    "    audio_image_preds.insert(i, 0)\n",
    "print(audio_image_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.load(\"/fsx/behavioural_computing_data/face_generation_data/1000actors_nsv/audio_emb/G023_C009_10258J_001.npy\").shape\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/1000actors_nsv/audio_emb/G023_C009_10258J_001_beats_emb.pt\"\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sample if ((i) % (14) != 0) or (i == 0) else \"bite\" for i, sample in enumerate(audio_image_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import profile\n",
    "\n",
    "# Assuming these functions are defined to work with your specific file format\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "path = \"/fsx/rs2517/data/HDTF/cropped_videos_original/WRA_VickyHartzler_000_video_512_latent.safetensors\"\n",
    "\n",
    "\n",
    "@profile\n",
    "def method1():\n",
    "    tensors = {}\n",
    "    with safe_open(path, framework=\"pt\") as f:\n",
    "        tensor_slice = f.get_slice(\"latents\")\n",
    "    print(\"Method 1: Tensor slice loaded.\")\n",
    "\n",
    "\n",
    "@profile\n",
    "def method2():\n",
    "    tensor = load_file(path)[\"latents\"]\n",
    "    print(\"Method 2: Entire tensor loaded.\")\n",
    "\n",
    "\n",
    "method1()\n",
    "method2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def logit_normal_sampler(m, s=1, beta_m=15, sample_num=1000000):\n",
    "    y_samples = torch.randn_like(m) * s + m\n",
    "    print(sample_num, s, (m + torch.randn(sample_num)).shape, torch.randn(sample_num).shape)\n",
    "    x_samples = beta_m * (torch.exp(y_samples) / (1 + torch.exp(y_samples)))\n",
    "    return x_samples\n",
    "\n",
    "\n",
    "def mu_t(t, a=5, mu_max=1):\n",
    "    t = t.to(\"cpu\")\n",
    "    return 2 * mu_max * t**a - mu_max\n",
    "\n",
    "\n",
    "def get_sigma_s(t, a, beta_m):\n",
    "    mu = mu_t(t, a=a)\n",
    "    sigma_s = logit_normal_sampler(m=mu, sample_num=t.shape[0], beta_m=beta_m)\n",
    "    return sigma_s\n",
    "\n",
    "\n",
    "sigma = torch.randn(2, 1, 1, 1)\n",
    "\n",
    "get_sigma_s(sigma, 5, 15).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
