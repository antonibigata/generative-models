{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "model_ckpt = load_file(\"../checkpoints/svd.safetensors\")\n",
    "\n",
    "print(model_ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([432, 2, 768])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "\n",
    "torch.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/1000actors_nsv/audio_emb/A435_C003_102997_001_wavlm_emb.pt\"\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/antoni/miniconda3/envs/svd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([0, 1, 1, 3]), {}, 0.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.io import read_video\n",
    "\n",
    "video, audio, info = read_video(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/AA_processed/part_8/video_crop/D550_C047_0106XS_001_output_output.mp4\"\n",
    ")\n",
    "video.shape, info, video.shape[0] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1912491]), 16000, 119.5306875)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "audio, sr = torchaudio.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/AA_processed/part_8/audio/D550_C047_0106XS_001.wav\"\n",
    ")\n",
    "audio.shape, sr, audio.shape[-1] / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\n",
    "    torch.load(\n",
    "        \"/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/audio_emb/1/-6yEyJRIUGc_5_0_0_170_beats_emb.pt\"\n",
    "    ).shape\n",
    ")\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "torch.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/audio_emb/1/-6yEyJRIUGc_5_0_0_170_beats_emb.pt\"\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/audio_emb/1/-6yEyJRIUGc_5_0_0_170_wav2vec2_emb.safetensors\"\n",
    ")[\"audio\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def max_divisions(a, b):\n",
    "    if b == 1:\n",
    "        return float(\"inf\")  # Dividing by 1 will never reduce the number\n",
    "    return math.floor(math.log(a, b))\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "a = 100\n",
    "b = 14\n",
    "print(max_divisions(a, b))  # Output: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\n",
    "    \"../logs/2024-08-21T12-15-52_example_training-svd_interpolation_no_emb/checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ckpt[\"module\"].keys():\n",
    "    if \"learned_mask\" in k:\n",
    "        print(k)\n",
    "        print(ckpt[\"module\"][k])\n",
    "        print(model_ckpt[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(\n",
    "    \"../logs/2024-05-30T18-28-54_example_training-svd_image/checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict[\"module\"].keys()\n",
    "# Take only weights that contain the word \"diffusion_model\" and keep only what comes after the word \"diffusion_model\" but including the word \"diffusion_model\"\n",
    "new_dict = {}\n",
    "for k, v in state_dict[\"module\"].items():\n",
    "    if \"diffusion_model\" in k:\n",
    "        new_dict[\"diffusion_model\" + k.split(\"diffusion_model\")[1]] = v\n",
    "new_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpth_path = \"/vol/bitbucket/abigata/checkpoints/trainstep_checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\"\n",
    "ckpt = torch.load(ckpth_path)\n",
    "\n",
    "print(ckpt[\"state_dict\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new weights for modified model\n",
    "new_weights = {}\n",
    "for k, v in model_ckpt.items():\n",
    "    # Remove model weights\n",
    "    # if k.startswith(\"model.\"):\n",
    "    #     continue\n",
    "    # Since embedder 1 and 2 have been removed, we need to remove the corresponding weights and change number of following embedders\n",
    "    if \"embedders.1\" in k or \"embedders.2\" in k:\n",
    "        print(\"chybrax\")\n",
    "        continue\n",
    "    if \"embedders.3\" in k:\n",
    "        new_weights[k.replace(\"embedders.3\", \"embedders.1\")] = v\n",
    "        continue\n",
    "    new_weights[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new weights\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(new_weights, \"../checkpoints/svd_no_emb.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Function to ensure all tensors are [1, 640]\n",
    "def ensure_shape(tensors, target_length=640):\n",
    "    processed_tensors = []\n",
    "    for tensor in tensors:\n",
    "        current_length = tensor.shape[1]\n",
    "        diff = current_length - target_length\n",
    "        assert abs(diff) <= 5, f\"Expected shape {target_length}, but got {current_length}\"\n",
    "        if diff < 0:\n",
    "            # Calculate how much padding is needed\n",
    "            padding_needed = target_length - current_length\n",
    "            # Pad the tensor\n",
    "            padded_tensor = F.pad(tensor, (0, padding_needed))\n",
    "            processed_tensors.append(padded_tensor)\n",
    "        elif diff > 0:\n",
    "            # Trim the tensor\n",
    "            trimmed_tensor = tensor[:, :target_length]\n",
    "            processed_tensors.append(trimmed_tensor)\n",
    "        else:\n",
    "            # If it's already the correct size\n",
    "            processed_tensors.append(tensor)\n",
    "    return torch.cat(processed_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decord import AVReader\n",
    "from decord import cpu, gpu\n",
    "import decord\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "import torch\n",
    "\n",
    "av = AVReader(\n",
    "    \"/vol/paramonos2/projects/antoni/code/Personal/generative-models/outputs/simple_video_sample/svd/000096_gt.mp4\",\n",
    "    ctx=cpu(0),\n",
    "    sample_rate=16000,\n",
    ")\n",
    "# To access both the video frames and corresponding audio samples\n",
    "audio, video = av[0:8]\n",
    "# for a in audio:\n",
    "#     print(a.shape)\n",
    "# audio = torch.cat(audio, dim=0)\n",
    "# # Each element in audio will be a batch of samples corresponding to a frame of video\n",
    "# print('Frame #: ', len(audio), len(video))\n",
    "# print('Shape of the audio samples of the first frame: ', audio.shape, len(audio))\n",
    "# print('Shape of the first frame: ', video[0].shape)\n",
    "# Similarly, to get a batch\n",
    "audio2, video2 = av.get_batch(range(0, len(av)))\n",
    "audio2 = ensure_shape(audio2)\n",
    "print(\"Shape of the audio samples of the first frame: \", audio2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av._AVReader__video_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "raw_audio = rearrange(audio2, \"t c -> (t c)\")\n",
    "raw_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "\n",
    "write(\"test.wav\", 16000, raw_audio.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot video\n",
    "import moviepy.editor as mp\n",
    "\n",
    "mp.VideoFileClip(\n",
    "    \"/vol/paramonos2/projects/antoni/code/Personal/generative-models/outputs/simple_video_sample/svd/000059.mp4\"\n",
    ").ipython_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import glob\n",
    "\n",
    "tar_files = glob.glob(\"/data/122-2/Datasets/CREMA/webdataset/train/batch_*.tar\")\n",
    "\n",
    "\n",
    "def preprocess_video(video):\n",
    "    # Your video preprocessing code here\n",
    "    return video\n",
    "\n",
    "\n",
    "def preprocess_audio(audio):\n",
    "    # Your audio preprocessing code here\n",
    "    return audio\n",
    "\n",
    "\n",
    "# Define how each type of file should be processed\n",
    "def process_sample(sample):\n",
    "    # Assuming the keys are 'video.mp4' and 'audio.wav' in your tar files\n",
    "    video, audio = sample[\"mp4\"], sample[\"wav\"]\n",
    "\n",
    "    # Apply your preprocessing functions\n",
    "    processed_video = preprocess_video(video)\n",
    "    processed_audio = preprocess_audio(audio)\n",
    "\n",
    "    return processed_video, processed_audio\n",
    "\n",
    "\n",
    "def custom_decoder(sample):\n",
    "    # Add custom decoding or handling for specific file types\n",
    "    # For example, if sample contains a '.wav' key, decode it appropriately\n",
    "    print(sample.keys())\n",
    "    return sample\n",
    "\n",
    "\n",
    "ds = wds.WebDataset(tar_files).map(custom_decoder)\n",
    "for sample in ds:\n",
    "    print(sample.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ffmpeg\n",
    "import tempfile\n",
    "\n",
    "\n",
    "class ResolutionSubsampler:\n",
    "    \"\"\"\n",
    "    Adjusts the resolution of the videos to the specified height and width.\n",
    "\n",
    "    Args:\n",
    "        video_size (int): Target resolution of the videos.\n",
    "        resize_mode (list[str]): List of resize modes to apply. Possible options are:\n",
    "            scale: scale video keeping aspect ratios (currently always picks video height)\n",
    "            crop: center crop to video_size x video_size\n",
    "            pad: center pad to video_size x video_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_size, resize_mode):\n",
    "        self.video_size = video_size\n",
    "        self.resize_mode = resize_mode\n",
    "\n",
    "    def __call__(self, streams):\n",
    "        video_bytes = streams[\"video\"]\n",
    "        subsampled_bytes = []\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            with open(os.path.join(tmpdir, \"input.mp4\"), \"wb\") as f:\n",
    "                f.write(video_bytes)\n",
    "            try:\n",
    "                _ = ffmpeg.input(f\"{tmpdir}/input.mp4\")\n",
    "                if \"scale\" in self.resize_mode:\n",
    "                    _ = _.filter(\"scale\", -2, self.video_size)\n",
    "                if \"crop\" in self.resize_mode:\n",
    "                    _ = _.filter(\"crop\", w=self.video_size, h=self.video_size)\n",
    "                if \"pad\" in self.resize_mode:\n",
    "                    _ = _.filter(\"pad\", w=self.video_size, h=self.video_size)\n",
    "                _ = _.output(f\"{tmpdir}/output.mp4\", reset_timestamps=1).run(capture_stdout=True, quiet=True)\n",
    "            except Exception as err:  # pylint: disable=broad-except\n",
    "                return [], None, str(err)\n",
    "\n",
    "            with open(f\"{tmpdir}/output.mp4\", \"rb\") as f:\n",
    "                subsampled_bytes = f.read()\n",
    "        streams[\"video\"] = subsampled_bytes\n",
    "        return streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_decoder(sample):\n",
    "    # Add custom decoding or handling for specific file types\n",
    "    # For example, if sample contains a '.wav' key, decode it appropriately\n",
    "    for key in sample.keys():\n",
    "        if key.endswith(\"video\"):\n",
    "            sample[key] = wds.torch_video(\"mp4\", sample[key])\n",
    "        if key.endswith(\"audio\"):\n",
    "            sample[key] = wds.torch_audio(\"wav\", sample[key])\n",
    "        elif key.endswith(\"audio_emb\"):\n",
    "            sample[key] = wds.torch_loads(sample[key])\n",
    "    return sample\n",
    "\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "i = 0\n",
    "ds = (\n",
    "    wds.WebDataset(\"/data/122-2/Datasets/CREMA/webdataset/train/out-{000000..000006}.tar\")\n",
    "    .rename(video=\"mp4;video\", audio=\"wav;audio\", audio_emb=\"pt;audio_emb\")\n",
    "    .map(ResolutionSubsampler(256, [\"scale\"]))\n",
    "    .map(custom_decoder)\n",
    ")\n",
    "for sample in ds:\n",
    "    print(sample.keys())\n",
    "    i += 1\n",
    "    # if i > 10:\n",
    "    break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "\n",
    "i = 0\n",
    "ds = wds.WebDataset(\"/data/122-2/Datasets/CREMA/webdataset/train/out-{000000..000006}.tar\").decode(\"torchl\")\n",
    "for sample in ds:\n",
    "    print(sample.keys())\n",
    "    # print(sample['mp4'][0].shape)\n",
    "    print(sample[\"pt\"].shape)\n",
    "    i += 1\n",
    "    # if i > 10:\n",
    "    break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/122-2/Datasets/CREMA/s32/audio/1032_IEO_FEA_LO_emb.pt\", \"rb\") as f:\n",
    "    video = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "\n",
    "wds.autodecode.decoders\n",
    "wds.autodecode.imagespecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdata import create_dataset, create_loader\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = OmegaConf.load(\"/vol/paramonos2/projects/antoni/code/Personal/generative-models/notebooks/data.yaml\")\n",
    "\n",
    "# build config\n",
    "datapipeline = create_dataset(**config.dataset)\n",
    "\n",
    "i = 0\n",
    "for sample in tqdm(datapipeline, desc=\"Loading dataset\"):\n",
    "    i += 1\n",
    "    break\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapipeline.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cond_frames\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cond = ((sample[\"cond_frames\"].permute(1, 2, 0).numpy() + 1) / 2).clip(0, 1) * 255\n",
    "cond = cond.astype(np.uint8)\n",
    "plt.imshow(cond)\n",
    "print(sample[\"cond_aug\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum\n",
    "\n",
    "scale = 1\n",
    "num_audio_frames = 28\n",
    "q = torch.randn(1, 14, 320)\n",
    "k = torch.randn(1, num_audio_frames, 320)\n",
    "v = torch.randn(1, num_audio_frames, 320)\n",
    "\n",
    "sim = einsum(\"b i d, b j d -> b i j\", q, k) * scale\n",
    "# del q, k\n",
    "\n",
    "# attention, what we cannot get enough of\n",
    "sim = sim.softmax(dim=-1)\n",
    "\n",
    "out = einsum(\"b i j, b j d -> b i d\", sim, v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = \"/vol/bitbucket/abigata/mp_rank_00_model_states.pt\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Attempt to load the .pt file\n",
    "    data = torch.load(checkpoint_path)\n",
    "    print(\"File loaded successfully. Integrity check passed.\")\n",
    "except Exception as e:\n",
    "    # Handle exceptions that indicate file loading issues\n",
    "    print(f\"Failed to load file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "sd = {k.replace(\"_forward_module.\", \"\"): v for k, v in checkpoint[\"module\"].items()}\n",
    "save_file(sd, checkpoint_path.replace(\".pt\", \".safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in checkpoint[\"module\"]:\n",
    "    if \"model.diffusion_model.input_blocks.1.0.time_stack.emb_layers.1.weight\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def select_indices(max_frames, n):\n",
    "    # Step 1: Randomly select the first index\n",
    "    index1 = random.randint(0, max_frames)\n",
    "\n",
    "    # Step 2: Determine the valid range for the second index\n",
    "    valid_range = list(range(0, index1 - n)) + list(range(index1 + n + 1, max_frames + 1))\n",
    "\n",
    "    # Check if the valid range is not empty\n",
    "    if not valid_range:\n",
    "        raise ValueError(\"The range is too small for these constraints!\")\n",
    "\n",
    "    # Step 3: Randomly select the second index from the valid range\n",
    "    index2 = random.choice(valid_range)\n",
    "\n",
    "    return index1, index2\n",
    "\n",
    "\n",
    "# Usage example\n",
    "max_frames = 100  # for example, 100 frames\n",
    "n = 10  # must be at least 10 indices apart\n",
    "indices = select_indices(max_frames, n)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/fsx/rs2517/data/HDTF/cropped_videos_original/WRA_ToddYoung_000_video_512_latent.safetensors\"\n",
    "tensors = {}\n",
    "with safe_open(path, framework=\"pt\") as f:\n",
    "    tensor_slice = f.get_slice(\"latents\")\n",
    "    print(tensor_slice.get_shape())\n",
    "    tensor = tensor_slice[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 25):\n",
    "    for j in range(i, 25):\n",
    "        if tensor[6250 + i].isclose(tensor[6250 + j]).all():\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_indexes(main_index, n_audio_frames, max_len):\n",
    "    # Get indexes for audio from both sides of the main index\n",
    "    audio_ids = []\n",
    "    # get audio embs from both sides of the GT frame\n",
    "    audio_ids += [0] * max(n_audio_frames - main_index, 0)\n",
    "    for i in range(max(main_index - n_audio_frames, 0), min(main_index + n_audio_frames + 1, max_len)):\n",
    "        # for i in range(frame_ids[0], min(frame_ids[0] + self.n_audio_motion_embs + 1, n_frames)):\n",
    "        audio_ids += [i]\n",
    "    audio_ids += [max_len - 1] * max(main_index + n_audio_frames - max_len + 1, 0)\n",
    "    return (audio_ids,)\n",
    "\n",
    "\n",
    "get_audio_indexes(27, 2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/fsx/rs2517/data/HDTF/audio/WRA_ToddYoung_000_wav2vec2_emb.safetensors\"\n",
    "tensors = {}\n",
    "with safe_open(path, framework=\"pt\") as f:\n",
    "    tensor_slice = f.get_slice(\"audio\")\n",
    "    print(tensor_slice.get_shape())\n",
    "tensor_slice[[0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "vr = decord.VideoReader(\"/fsx/rs2517/data/HDTF/cropped_videos_original/WRA_ToddYoung_000.mp4\")\n",
    "print(len(vr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "valid_files = 0\n",
    "files = glob.glob(\"/fsx/rs2517/data/HDTF/cropped_videos_original/*.safetensors\")\n",
    "for file in files:\n",
    "    with safe_open(file, framework=\"pt\") as f:\n",
    "        tensor_slice = f.get_slice(\"latents\")\n",
    "        latent_shape = tensor_slice.get_shape()[0]\n",
    "\n",
    "    file = file.replace(\"cropped_videos_original\", \"audio\").replace(\"video_512_latent\", \"wav2vec2_emb\")\n",
    "    with safe_open(file, framework=\"pt\") as f:\n",
    "        tensor_slice = f.get_slice(\"audio\")\n",
    "        audio_shape = tensor_slice.get_shape()[0]\n",
    "\n",
    "    if latent_shape != audio_shape:\n",
    "        print(\n",
    "            f\"latent_shape: {latent_shape}, audio_shape: {audio_shape}. Difference: {abs(latent_shape - audio_shape)}\"\n",
    "        )\n",
    "    else:\n",
    "        valid_files += 1\n",
    "print(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "scale = torch.linspace(5, 2, 14).unsqueeze(0)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_image_preds = [i for i in range(0, 15 * 25, 13)]\n",
    "print(audio_image_preds)\n",
    "len_audio_image_preds = len(audio_image_preds)\n",
    "print(len_audio_image_preds)\n",
    "for i in range(14, len_audio_image_preds + len_audio_image_preds % 14, 14):\n",
    "    print(i)\n",
    "    audio_image_preds.insert(i, 0)\n",
    "print(audio_image_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.load(\"/fsx/behavioural_computing_data/face_generation_data/1000actors_nsv/audio_emb/G023_C009_10258J_001.npy\").shape\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/1000actors_nsv/audio_emb/G023_C009_10258J_001_beats_emb.pt\"\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sample if ((i) % (14) != 0) or (i == 0) else \"bite\" for i, sample in enumerate(audio_image_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import profile\n",
    "\n",
    "# Assuming these functions are defined to work with your specific file format\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "path = \"/fsx/rs2517/data/HDTF/cropped_videos_original/WRA_VickyHartzler_000_video_512_latent.safetensors\"\n",
    "\n",
    "\n",
    "@profile\n",
    "def method1():\n",
    "    tensors = {}\n",
    "    with safe_open(path, framework=\"pt\") as f:\n",
    "        tensor_slice = f.get_slice(\"latents\")\n",
    "    print(\"Method 1: Tensor slice loaded.\")\n",
    "\n",
    "\n",
    "@profile\n",
    "def method2():\n",
    "    tensor = load_file(path)[\"latents\"]\n",
    "    print(\"Method 2: Entire tensor loaded.\")\n",
    "\n",
    "\n",
    "method1()\n",
    "method2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def logit_normal_sampler(m, s=1, beta_m=15, sample_num=1000000):\n",
    "    y_samples = torch.randn_like(m) * s + m\n",
    "    print(sample_num, s, (m + torch.randn(sample_num)).shape, torch.randn(sample_num).shape)\n",
    "    x_samples = beta_m * (torch.exp(y_samples) / (1 + torch.exp(y_samples)))\n",
    "    return x_samples\n",
    "\n",
    "\n",
    "def mu_t(t, a=5, mu_max=1):\n",
    "    t = t.to(\"cpu\")\n",
    "    return 2 * mu_max * t**a - mu_max\n",
    "\n",
    "\n",
    "def get_sigma_s(t, a, beta_m):\n",
    "    mu = mu_t(t, a=a)\n",
    "    sigma_s = logit_normal_sampler(m=mu, sample_num=t.shape[0], beta_m=beta_m)\n",
    "    return sigma_s\n",
    "\n",
    "\n",
    "sigma = torch.randn(2, 1, 1, 1)\n",
    "\n",
    "get_sigma_s(sigma, 5, 15).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected 100 video paths have been saved to /data/home/antoni/code/generative-models/selected_100_video_paths.txt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Path to the file containing the video paths\n",
    "file_path = \"/data/home/antoni/code/generative-models/CelebV-Text_longest_samples.txt\"\n",
    "\n",
    "\n",
    "# Function to read the file and extract video paths\n",
    "def read_video_paths(file_path):\n",
    "    video_paths = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            if \"Duration:\" in line and line.split(\".\")[0].isdigit():\n",
    "                # Extract the video path from the line\n",
    "                path = line.split(\"Path: \")[-1].strip()\n",
    "                video_paths.append(path)\n",
    "    return video_paths\n",
    "\n",
    "\n",
    "# Read all video paths\n",
    "all_video_paths = read_video_paths(file_path)\n",
    "\n",
    "# Randomly select 100 video paths\n",
    "selected_paths = random.sample(all_video_paths, min(100, len(all_video_paths)))\n",
    "\n",
    "# Save the selected paths to a new file\n",
    "output_file = \"/data/home/antoni/code/generative-models/selected_100_video_paths.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    for path in selected_paths:\n",
    "        file.write(f\"{path}\\n\")\n",
    "\n",
    "print(f\"Randomly selected 100 video paths have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/551/YM1MXvBs5Pw_1_0_0_11348.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/255/ImdlJqjT0vE_2_1_0_7670.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/142/9nwULlly2g4_0_0_551_7595.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/18/0PnWeFoWG1s_0_0_0_6663.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/527/oQgourbirio_0_0_3646_9476.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/385/UoL4eD4x-SE_18_0_221_5812.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/288/Lbje1PTEGB8_0_0_5018_10375.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/88/69IuXpgBEVM_1_1_0_5303.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/365/SyYbSk4oN8Y_0_0_1900_7164.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/327/OzUJYf3iBxA_0_0_8954_14080.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/618/tps-SR1aS4U_5_0_1925_7025.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/201/DylzIymayp4_0_4_14_4834.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/288/Lbje1PTEGB8_0_0_575_5000.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/209/EorZimlGxGg_7_0_0_4336.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/489/w62_EIBxcSc_4_0_4686_9000.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/489/w62_EIBxcSc_4_0_424_4675.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/156/Axp8RZmaouk_2_1_0_4200.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/641/_D1Gccp6RGk_6_0_0_4148.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/51/3HyVA5b3w_0_5_0_2650_6674.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/288/Lbje1PTEGB8_0_0_10450_14309.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/45/2z-K3GfCuLs_4_0_0_3824.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/270/K-XgW7gHVkY_1_1_1667_5300.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/334/Pn2KCBIP-C0_11_0_3970_7596.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/440/yY8KlKUleSo_0_0_0_3544.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/663/c0o2h2gKqkY_14_0_0_3358.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/327/OzUJYf3iBxA_0_0_2825_6137.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/633/cmmIr6Dc8ag_28_0_10189_13500.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/527/oQgourbirio_0_0_9975_13283.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/618/tps-SR1aS4U_5_0_7050_10266.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/209/EorZimlGxGg_9_0_0_3187.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/15/0AZYAWbB0mg_19_0_713_3900.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/241/HPNjNfRYmg8_1_1_0_2928.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/352/RcHc-da-9Cg_2_0_0_2871.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/204/EDKQLgXR4ic_4_0_1_2871.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/64/4RDuVLSNI4s_0_0_0_2835.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/620/z5-8idQpgD4_1_0_0_2820.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/34/1jugmjpAWnI_0_1_0_2789.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/414/XMKDXmDnNSU_0_0_0_2729.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/632/a9rN6Ugbf-w_5_0_0_2724.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/116/809HGrjPEDI_8_1_0_2724.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/60/3zI7c0NGSik_1_1_139_2778.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/644/hD99GNn-Cxo_1_5_0_2634.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/672/b7qyqdZRDYI_9_0_0_2623.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/359/SLNgcYfGtzo_0_1_0_2619.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/27/1CFYs69C6C4_12_0_44_2645.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/61/43bZxR0SYqg_2_0_0_2570.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/16/0E3I5e4HyCg_13_0_112_2672.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/426/mAjmqBrzBJQ_15_0_0_2550.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/389/V8tNZpUkOGk_1_0_0_2549.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/441/Y9tW73feMwc_7_2_0_2547.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/568/sSFmH6Jsp3c_1_1_0_2546.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/51/3HyVA5b3w_0_2_0_1448_3974.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/527/oQgourbirio_0_0_983_3502.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/530/yggKNhEHzH4_2_0_0_2516.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/670/xm2O5UkcyvI_6_1_0_2515.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/531/YSpF9PfDDmw_16_1_18_2528.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/62/48qItnBcog0_5_4_0_2504.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/148/AQLA6iwp7Ew_5_1_0_2502.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/576/npoDeq8yOh4_17_0_250_2750.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/365/T1E9s8MsiVo_5_0_0_2493.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/579/v9UPcurcwQI_36_0_0_2475.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/304/N3K7sgp2X74_1_0_15_2490.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/376/U-5M5__7oRM_6_0_677_3139.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/457/pj4ZOgdX5BU_30_0_0_2448.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/684/hD99GNn-Cxo_1_0_0_2443.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/478/tc2q5EG1tAQ_2_0_261_2700.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/138/9eLn6Xz_bpg_39_1_0_2421.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/410/zR_Vb_lkgyo_35_1_0_2405.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/13/0-qhEXtUc50_1_0_0_2400.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/633/cmmIr6Dc8ag_28_0_3927_6323.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/135/9OznLvHKdxo_1_6_0_2377.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/276/KanJotZcifA_24_0_0_2356.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/82/5gUkXx1Xss4_1_0_0_2350.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/168/BvKRycZ_Sr8_9_0_0_2342.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/125/8d9iuo-Ck14_13_0_0_2332.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/483/dBUUCICRTvU_9_0_0_2330.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/644/fiPU9igUTDU_3_0_0_2328.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/108/7Y5gWfsOfFw_3_1_0_2325.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/690/zhCf3moJj1s_9_0_0_2316.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/455/jO_p0EuwbU4_0_0_9_2325.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/68/4funeuVTGp0_0_0_0_2312.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/251/IJRXShDE_ak_4_2_0_2307.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/35/1sYXVZpeQcA_1_1_0_2300.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/372/Tf7-zk1siVI_15_0_467_2761.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/180/pX-OfdMVGeU_8_10_21_2310.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/94/6XmQejFQXtk_3_1_0_2282.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/506/npoDeq8yOh4_18_0_0_2277.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/325/OvS1D6hbLS0_5_0_25_2300.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/205/EMR1TfRnD_Q_3_0_38_2307.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/131/94-zmvGI1SQ_3_1_0_2252.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/261/J8hCbto-zqU_2_1_617_2868.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/232/Gi4r2pPy9jI_0_0_0_2248.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/48/3C1XwpJ1WRw_7_1_668_2913.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/420/z41jiPgsvNg_20_4_0_2241.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/492/_D1Gccp6RGk_2_0_0_2238.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/87/62tOh3v_dpA_5_0_0_2231.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/49/3HyVA5b3w_0_1_0_0_2213.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/402/WDJG6Zn1CGw_4_1_0_2210.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/354/RlrmSu5u9bE_4_0_0_2210.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/617/pMxYYYZ04nc_11_0_4_2212.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/340/QOhGEk4xW6g_6_4_0_2202.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/433/egJgsDLWJXs_4_7_0_2181.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/40/rfJNmpOc6gs_4_0_323_2502.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/414/XJK_qf1shuA_2_0_0_2175.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/423/e2HyW2ek5MA_6_0_520_2694.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/327/OzUJYf3iBxA_0_0_6551_8715.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/633/egJgsDLWJXs_6_0_0_2163.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/220/y8HaEYebwFo_71_0_0_2159.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/550/yajNjLPmnCQ_4_0_0_2147.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/85/5t-eheKJhVM_2_0_0_2133.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/197/Dk9ZrJU2ZwE_11_0_0_2129.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/147/AEQP0fukIfQ_3_0_0_2128.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/131/94-zmvGI1SQ_3_0_0_2125.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/478/rfJNmpOc6gs_16_1_0_2123.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/640/z5-8idQpgD4_0_0_0_2121.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/144/9y0r_aEg2Kw_17_3_0_2109.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/340/QRFpvfqgJEY_0_0_25_2132.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/158/B5BbbIUhzVY_7_0_0_2102.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/648/rnry5iUieps_15_0_0_2100.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/211/EsDLU1W9W_Y_9_0_0_2100.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/660/QjonHuFcvrk_19_0_412_2510.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/12/-uz0J8fq_T8_21_0_280_2364.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/521/YDUlSk65R5o_2_0_0_2083.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/236/H7zr5Z-sAdc_0_0_0_2080.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/504/fAq1Wb5QfQ4_0_0_0_2078.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/686/nB_s79lHSVs_11_0_0_2073.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/676/lwbnxUmAqEA_1_0_0_2065.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/633/cmmIr6Dc8ag_28_0_8089_10150.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/477/qP1R6t9PFbA_28_0_0_2057.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/445/iYvWdRrYmKQ_9_0_26_2077.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/75/5E89iWXIgnA_1_0_1066_3106.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/653/e2HyW2ek5MA_9_0_337_2367.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/492/aECKQ1JtfiE_0_0_0_2030.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/425/iKzEBgAj8eE_23_0_0_2022.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/490/xw2dLIlmBNk_1_1_0_1997.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/393/VYP2MF3r3a0_22_0_0_1997.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/81/5_crULFqUA0_14_3_0_1990.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/282/KyjizZ7Y5ic_9_2_0_1983.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/562/bajaV1doxKU_4_0_1136_3114.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/451/Y927vRQrRno_6_0_0_1976.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/63/4JqvI-BrFAE_1_0_13307_15276.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/649/v9UPcurcwQI_49_0_1_1962.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/304/N3K7sgp2X74_5_0_0_1958.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/527/qwywVQxKkvI_0_0_0_1950.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/290/sYQqhEWGrp8_0_0_0_1950.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/244/HkvJ9u6c4Vk_9_1_0_1943.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/676/mVMcIym7y5U_2_0_0_1942.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/423/e2HyW2ek5MA_10_0_0_1939.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/26/12YoSHvEAx8_5_0_0_1935.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/552/_zg-TXKKoGE_20_0_1323_3255.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/81/5aMNGQKBZwo_0_0_0_1926.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/72/4zrWpbuwC-Q_0_0_25_1950.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/615/k-3f0NpRuKU_11_0_0_1924.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/115/7tCU3GeNdHE_22_0_0_1924.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/500/y02mu9y3ZAQ_17_0_5_1926.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/534/gpYudUVSako_0_0_0_1916.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/619/veh4QNofvgg_7_1_0_1914.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/502/afUuQcrcy90_7_0_486_2396.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/177/CXjfORmQEfU_12_0_0_1906.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/685/jq57YU6z8AU_28_0_0_1902.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/618/tps-SR1aS4U_5_0_0_1900.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/334/Pn2KCBIP-C0_4_0_0_1900.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/68/4funeuVTGp0_1_0_2221_4120.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/258/J5QpQHukyPs_5_0_0_1899.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/244/HkvJ9u6c4Vk_13_0_0_1889.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/211/EsDLU1W9W_Y_5_0_0_1887.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/210/EsDLU1W9W_Y_10_0_0_1886.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/660/yJqR66bVP94_12_3_125_2001.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/45/30O6x6K3v4M_1_0_2457_4331.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/337/Q1QME3rBAYk_10_0_0_1870.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/92/6PLLoixgamY_11_0_0_1865.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/576/lTu8MIfKZIQ_24_0_0_1857.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/465/ijv8jSJc33E_20_0_0_1853.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/379/UKIFugom_0Y_9_0_0_1852.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/325/OrYYQvPilSk_0_0_0_1852.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/5/-OENibO6Fx8_28_2_0_1850.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/687/pIHTnpXmRKw_19_2_0_1849.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/274/KODMxfpjiOo_0_0_1_1850.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/133/98e4gYIWKVY_10_0_0_1848.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/282/L2QUf4neMiE_3_0_319_2166.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/45/30O6x6K3v4M_1_0_0_1844.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/23/0iEQk-BAvrk_5_0_1795_3625.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/317/OBLuzI9aaEQ_11_0_0_1828.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/495/kvMod1S-8Tk_18_0_507_2334.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/420/xlmI9kOQ3ZI_2_0_0_1825.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/641/ZxdjZcTDNTs_33_2_0_1817.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/205/EMR1TfRnD_Q_8_0_9_1822.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/282/KyjizZ7Y5ic_12_3_0_1808.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/420/xWJU26B3GZQ_2_0_1_1805.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/516/m3uOregkQjc_0_0_0_1800.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/319/OK9_bD5wn0I_9_0_260_2057.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/651/Z0-oT7H07Kc_1_0_0_1789.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/185/D-4VvAQb01Y_0_0_12_1800.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/196/DfhZN5nj80g_6_0_0_1783.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/480/xhLqkwkVfGo_11_0_0_1781.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/4/-IvUXcLbgoc_4_0_0_1765.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/406/WaW319TjU-c_2_0_22_1785.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/206/EYYMevrp3So_17_0_11_1773.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/593/e2HyW2ek5MA_2_0_270_2027.mp4',\n",
       " '/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/video_crop/108/7Y-eKur822k_0_0_0_1756.mp4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_video_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
