{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "model_ckpt = load_file(\"../checkpoints/svd.safetensors\")\n",
    "\n",
    "print(model_ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([432, 2, 768])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "\n",
    "torch.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/1000actors_nsv/audio_emb/A435_C003_102997_001_wavlm_emb.pt\"\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/antoni/miniconda3/envs/svd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([0, 1, 1, 3]), {}, 0.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.io import read_video\n",
    "\n",
    "video, audio, info = read_video(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/AA_processed/part_8/video_crop/D550_C047_0106XS_001_output_output.mp4\"\n",
    ")\n",
    "video.shape, info, video.shape[0] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1912491]), 16000, 119.5306875)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "audio, sr = torchaudio.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/AA_processed/part_8/audio/D550_C047_0106XS_001.wav\"\n",
    ")\n",
    "audio.shape, sr, audio.shape[-1] / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\n",
    "    torch.load(\n",
    "        \"/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/audio_emb/1/-6yEyJRIUGc_5_0_0_170_beats_emb.pt\"\n",
    "    ).shape\n",
    ")\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "torch.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/audio_emb/1/-6yEyJRIUGc_5_0_0_170_beats_emb.pt\"\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/CelebV-Text/audio_emb/1/-6yEyJRIUGc_5_0_0_170_wav2vec2_emb.safetensors\"\n",
    ")[\"audio\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def max_divisions(a, b):\n",
    "    if b == 1:\n",
    "        return float(\"inf\")  # Dividing by 1 will never reduce the number\n",
    "    return math.floor(math.log(a, b))\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "a = 100\n",
    "b = 14\n",
    "print(max_divisions(a, b))  # Output: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\n",
    "    \"../logs/2024-08-21T12-15-52_example_training-svd_interpolation_no_emb/checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ckpt[\"module\"].keys():\n",
    "    if \"learned_mask\" in k:\n",
    "        print(k)\n",
    "        print(ckpt[\"module\"][k])\n",
    "        print(model_ckpt[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(\n",
    "    \"../logs/2024-05-30T18-28-54_example_training-svd_image/checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict[\"module\"].keys()\n",
    "# Take only weights that contain the word \"diffusion_model\" and keep only what comes after the word \"diffusion_model\" but including the word \"diffusion_model\"\n",
    "new_dict = {}\n",
    "for k, v in state_dict[\"module\"].items():\n",
    "    if \"diffusion_model\" in k:\n",
    "        new_dict[\"diffusion_model\" + k.split(\"diffusion_model\")[1]] = v\n",
    "new_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpth_path = \"/vol/bitbucket/abigata/checkpoints/trainstep_checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\"\n",
    "ckpt = torch.load(ckpth_path)\n",
    "\n",
    "print(ckpt[\"state_dict\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new weights for modified model\n",
    "new_weights = {}\n",
    "for k, v in model_ckpt.items():\n",
    "    # Remove model weights\n",
    "    # if k.startswith(\"model.\"):\n",
    "    #     continue\n",
    "    # Since embedder 1 and 2 have been removed, we need to remove the corresponding weights and change number of following embedders\n",
    "    if \"embedders.1\" in k or \"embedders.2\" in k:\n",
    "        print(\"chybrax\")\n",
    "        continue\n",
    "    if \"embedders.3\" in k:\n",
    "        new_weights[k.replace(\"embedders.3\", \"embedders.1\")] = v\n",
    "        continue\n",
    "    new_weights[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new weights\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(new_weights, \"../checkpoints/svd_no_emb.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Function to ensure all tensors are [1, 640]\n",
    "def ensure_shape(tensors, target_length=640):\n",
    "    processed_tensors = []\n",
    "    for tensor in tensors:\n",
    "        current_length = tensor.shape[1]\n",
    "        diff = current_length - target_length\n",
    "        assert abs(diff) <= 5, f\"Expected shape {target_length}, but got {current_length}\"\n",
    "        if diff < 0:\n",
    "            # Calculate how much padding is needed\n",
    "            padding_needed = target_length - current_length\n",
    "            # Pad the tensor\n",
    "            padded_tensor = F.pad(tensor, (0, padding_needed))\n",
    "            processed_tensors.append(padded_tensor)\n",
    "        elif diff > 0:\n",
    "            # Trim the tensor\n",
    "            trimmed_tensor = tensor[:, :target_length]\n",
    "            processed_tensors.append(trimmed_tensor)\n",
    "        else:\n",
    "            # If it's already the correct size\n",
    "            processed_tensors.append(tensor)\n",
    "    return torch.cat(processed_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decord import AVReader\n",
    "from decord import cpu, gpu\n",
    "import decord\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "import torch\n",
    "\n",
    "av = AVReader(\n",
    "    \"/vol/paramonos2/projects/antoni/code/Personal/generative-models/outputs/simple_video_sample/svd/000096_gt.mp4\",\n",
    "    ctx=cpu(0),\n",
    "    sample_rate=16000,\n",
    ")\n",
    "# To access both the video frames and corresponding audio samples\n",
    "audio, video = av[0:8]\n",
    "# for a in audio:\n",
    "#     print(a.shape)\n",
    "# audio = torch.cat(audio, dim=0)\n",
    "# # Each element in audio will be a batch of samples corresponding to a frame of video\n",
    "# print('Frame #: ', len(audio), len(video))\n",
    "# print('Shape of the audio samples of the first frame: ', audio.shape, len(audio))\n",
    "# print('Shape of the first frame: ', video[0].shape)\n",
    "# Similarly, to get a batch\n",
    "audio2, video2 = av.get_batch(range(0, len(av)))\n",
    "audio2 = ensure_shape(audio2)\n",
    "print(\"Shape of the audio samples of the first frame: \", audio2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av._AVReader__video_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "raw_audio = rearrange(audio2, \"t c -> (t c)\")\n",
    "raw_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "\n",
    "write(\"test.wav\", 16000, raw_audio.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot video\n",
    "import moviepy.editor as mp\n",
    "\n",
    "mp.VideoFileClip(\n",
    "    \"/vol/paramonos2/projects/antoni/code/Personal/generative-models/outputs/simple_video_sample/svd/000059.mp4\"\n",
    ").ipython_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import glob\n",
    "\n",
    "tar_files = glob.glob(\"/data/122-2/Datasets/CREMA/webdataset/train/batch_*.tar\")\n",
    "\n",
    "\n",
    "def preprocess_video(video):\n",
    "    # Your video preprocessing code here\n",
    "    return video\n",
    "\n",
    "\n",
    "def preprocess_audio(audio):\n",
    "    # Your audio preprocessing code here\n",
    "    return audio\n",
    "\n",
    "\n",
    "# Define how each type of file should be processed\n",
    "def process_sample(sample):\n",
    "    # Assuming the keys are 'video.mp4' and 'audio.wav' in your tar files\n",
    "    video, audio = sample[\"mp4\"], sample[\"wav\"]\n",
    "\n",
    "    # Apply your preprocessing functions\n",
    "    processed_video = preprocess_video(video)\n",
    "    processed_audio = preprocess_audio(audio)\n",
    "\n",
    "    return processed_video, processed_audio\n",
    "\n",
    "\n",
    "def custom_decoder(sample):\n",
    "    # Add custom decoding or handling for specific file types\n",
    "    # For example, if sample contains a '.wav' key, decode it appropriately\n",
    "    print(sample.keys())\n",
    "    return sample\n",
    "\n",
    "\n",
    "ds = wds.WebDataset(tar_files).map(custom_decoder)\n",
    "for sample in ds:\n",
    "    print(sample.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ffmpeg\n",
    "import tempfile\n",
    "\n",
    "\n",
    "class ResolutionSubsampler:\n",
    "    \"\"\"\n",
    "    Adjusts the resolution of the videos to the specified height and width.\n",
    "\n",
    "    Args:\n",
    "        video_size (int): Target resolution of the videos.\n",
    "        resize_mode (list[str]): List of resize modes to apply. Possible options are:\n",
    "            scale: scale video keeping aspect ratios (currently always picks video height)\n",
    "            crop: center crop to video_size x video_size\n",
    "            pad: center pad to video_size x video_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_size, resize_mode):\n",
    "        self.video_size = video_size\n",
    "        self.resize_mode = resize_mode\n",
    "\n",
    "    def __call__(self, streams):\n",
    "        video_bytes = streams[\"video\"]\n",
    "        subsampled_bytes = []\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            with open(os.path.join(tmpdir, \"input.mp4\"), \"wb\") as f:\n",
    "                f.write(video_bytes)\n",
    "            try:\n",
    "                _ = ffmpeg.input(f\"{tmpdir}/input.mp4\")\n",
    "                if \"scale\" in self.resize_mode:\n",
    "                    _ = _.filter(\"scale\", -2, self.video_size)\n",
    "                if \"crop\" in self.resize_mode:\n",
    "                    _ = _.filter(\"crop\", w=self.video_size, h=self.video_size)\n",
    "                if \"pad\" in self.resize_mode:\n",
    "                    _ = _.filter(\"pad\", w=self.video_size, h=self.video_size)\n",
    "                _ = _.output(f\"{tmpdir}/output.mp4\", reset_timestamps=1).run(capture_stdout=True, quiet=True)\n",
    "            except Exception as err:  # pylint: disable=broad-except\n",
    "                return [], None, str(err)\n",
    "\n",
    "            with open(f\"{tmpdir}/output.mp4\", \"rb\") as f:\n",
    "                subsampled_bytes = f.read()\n",
    "        streams[\"video\"] = subsampled_bytes\n",
    "        return streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_decoder(sample):\n",
    "    # Add custom decoding or handling for specific file types\n",
    "    # For example, if sample contains a '.wav' key, decode it appropriately\n",
    "    for key in sample.keys():\n",
    "        if key.endswith(\"video\"):\n",
    "            sample[key] = wds.torch_video(\"mp4\", sample[key])\n",
    "        if key.endswith(\"audio\"):\n",
    "            sample[key] = wds.torch_audio(\"wav\", sample[key])\n",
    "        elif key.endswith(\"audio_emb\"):\n",
    "            sample[key] = wds.torch_loads(sample[key])\n",
    "    return sample\n",
    "\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "i = 0\n",
    "ds = (\n",
    "    wds.WebDataset(\"/data/122-2/Datasets/CREMA/webdataset/train/out-{000000..000006}.tar\")\n",
    "    .rename(video=\"mp4;video\", audio=\"wav;audio\", audio_emb=\"pt;audio_emb\")\n",
    "    .map(ResolutionSubsampler(256, [\"scale\"]))\n",
    "    .map(custom_decoder)\n",
    ")\n",
    "for sample in ds:\n",
    "    print(sample.keys())\n",
    "    i += 1\n",
    "    # if i > 10:\n",
    "    break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "\n",
    "i = 0\n",
    "ds = wds.WebDataset(\"/data/122-2/Datasets/CREMA/webdataset/train/out-{000000..000006}.tar\").decode(\"torchl\")\n",
    "for sample in ds:\n",
    "    print(sample.keys())\n",
    "    # print(sample['mp4'][0].shape)\n",
    "    print(sample[\"pt\"].shape)\n",
    "    i += 1\n",
    "    # if i > 10:\n",
    "    break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/122-2/Datasets/CREMA/s32/audio/1032_IEO_FEA_LO_emb.pt\", \"rb\") as f:\n",
    "    video = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "\n",
    "wds.autodecode.decoders\n",
    "wds.autodecode.imagespecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdata import create_dataset, create_loader\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = OmegaConf.load(\"/vol/paramonos2/projects/antoni/code/Personal/generative-models/notebooks/data.yaml\")\n",
    "\n",
    "# build config\n",
    "datapipeline = create_dataset(**config.dataset)\n",
    "\n",
    "i = 0\n",
    "for sample in tqdm(datapipeline, desc=\"Loading dataset\"):\n",
    "    i += 1\n",
    "    break\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapipeline.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cond_frames\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cond = ((sample[\"cond_frames\"].permute(1, 2, 0).numpy() + 1) / 2).clip(0, 1) * 255\n",
    "cond = cond.astype(np.uint8)\n",
    "plt.imshow(cond)\n",
    "print(sample[\"cond_aug\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum\n",
    "\n",
    "scale = 1\n",
    "num_audio_frames = 28\n",
    "q = torch.randn(1, 14, 320)\n",
    "k = torch.randn(1, num_audio_frames, 320)\n",
    "v = torch.randn(1, num_audio_frames, 320)\n",
    "\n",
    "sim = einsum(\"b i d, b j d -> b i j\", q, k) * scale\n",
    "# del q, k\n",
    "\n",
    "# attention, what we cannot get enough of\n",
    "sim = sim.softmax(dim=-1)\n",
    "\n",
    "out = einsum(\"b i j, b j d -> b i d\", sim, v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = \"/vol/bitbucket/abigata/mp_rank_00_model_states.pt\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Attempt to load the .pt file\n",
    "    data = torch.load(checkpoint_path)\n",
    "    print(\"File loaded successfully. Integrity check passed.\")\n",
    "except Exception as e:\n",
    "    # Handle exceptions that indicate file loading issues\n",
    "    print(f\"Failed to load file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "sd = {k.replace(\"_forward_module.\", \"\"): v for k, v in checkpoint[\"module\"].items()}\n",
    "save_file(sd, checkpoint_path.replace(\".pt\", \".safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in checkpoint[\"module\"]:\n",
    "    if \"model.diffusion_model.input_blocks.1.0.time_stack.emb_layers.1.weight\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def select_indices(max_frames, n):\n",
    "    # Step 1: Randomly select the first index\n",
    "    index1 = random.randint(0, max_frames)\n",
    "\n",
    "    # Step 2: Determine the valid range for the second index\n",
    "    valid_range = list(range(0, index1 - n)) + list(range(index1 + n + 1, max_frames + 1))\n",
    "\n",
    "    # Check if the valid range is not empty\n",
    "    if not valid_range:\n",
    "        raise ValueError(\"The range is too small for these constraints!\")\n",
    "\n",
    "    # Step 3: Randomly select the second index from the valid range\n",
    "    index2 = random.choice(valid_range)\n",
    "\n",
    "    return index1, index2\n",
    "\n",
    "\n",
    "# Usage example\n",
    "max_frames = 100  # for example, 100 frames\n",
    "n = 10  # must be at least 10 indices apart\n",
    "indices = select_indices(max_frames, n)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/fsx/rs2517/data/HDTF/cropped_videos_original/WRA_ToddYoung_000_video_512_latent.safetensors\"\n",
    "tensors = {}\n",
    "with safe_open(path, framework=\"pt\") as f:\n",
    "    tensor_slice = f.get_slice(\"latents\")\n",
    "    print(tensor_slice.get_shape())\n",
    "    tensor = tensor_slice[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 25):\n",
    "    for j in range(i, 25):\n",
    "        if tensor[6250 + i].isclose(tensor[6250 + j]).all():\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_indexes(main_index, n_audio_frames, max_len):\n",
    "    # Get indexes for audio from both sides of the main index\n",
    "    audio_ids = []\n",
    "    # get audio embs from both sides of the GT frame\n",
    "    audio_ids += [0] * max(n_audio_frames - main_index, 0)\n",
    "    for i in range(max(main_index - n_audio_frames, 0), min(main_index + n_audio_frames + 1, max_len)):\n",
    "        # for i in range(frame_ids[0], min(frame_ids[0] + self.n_audio_motion_embs + 1, n_frames)):\n",
    "        audio_ids += [i]\n",
    "    audio_ids += [max_len - 1] * max(main_index + n_audio_frames - max_len + 1, 0)\n",
    "    return (audio_ids,)\n",
    "\n",
    "\n",
    "get_audio_indexes(27, 2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/fsx/rs2517/data/HDTF/audio/WRA_ToddYoung_000_wav2vec2_emb.safetensors\"\n",
    "tensors = {}\n",
    "with safe_open(path, framework=\"pt\") as f:\n",
    "    tensor_slice = f.get_slice(\"audio\")\n",
    "    print(tensor_slice.get_shape())\n",
    "tensor_slice[[0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "vr = decord.VideoReader(\"/fsx/rs2517/data/HDTF/cropped_videos_original/WRA_ToddYoung_000.mp4\")\n",
    "print(len(vr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "valid_files = 0\n",
    "files = glob.glob(\"/fsx/rs2517/data/HDTF/cropped_videos_original/*.safetensors\")\n",
    "for file in files:\n",
    "    with safe_open(file, framework=\"pt\") as f:\n",
    "        tensor_slice = f.get_slice(\"latents\")\n",
    "        latent_shape = tensor_slice.get_shape()[0]\n",
    "\n",
    "    file = file.replace(\"cropped_videos_original\", \"audio\").replace(\"video_512_latent\", \"wav2vec2_emb\")\n",
    "    with safe_open(file, framework=\"pt\") as f:\n",
    "        tensor_slice = f.get_slice(\"audio\")\n",
    "        audio_shape = tensor_slice.get_shape()[0]\n",
    "\n",
    "    if latent_shape != audio_shape:\n",
    "        print(\n",
    "            f\"latent_shape: {latent_shape}, audio_shape: {audio_shape}. Difference: {abs(latent_shape - audio_shape)}\"\n",
    "        )\n",
    "    else:\n",
    "        valid_files += 1\n",
    "print(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "scale = torch.linspace(5, 2, 14).unsqueeze(0)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_image_preds = [i for i in range(0, 15 * 25, 13)]\n",
    "print(audio_image_preds)\n",
    "len_audio_image_preds = len(audio_image_preds)\n",
    "print(len_audio_image_preds)\n",
    "for i in range(14, len_audio_image_preds + len_audio_image_preds % 14, 14):\n",
    "    print(i)\n",
    "    audio_image_preds.insert(i, 0)\n",
    "print(audio_image_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.load(\"/fsx/behavioural_computing_data/face_generation_data/1000actors_nsv/audio_emb/G023_C009_10258J_001.npy\").shape\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.load(\n",
    "    \"/fsx/behavioural_computing_data/face_generation_data/1000actors_nsv/audio_emb/G023_C009_10258J_001_beats_emb.pt\"\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sample if ((i) % (14) != 0) or (i == 0) else \"bite\" for i, sample in enumerate(audio_image_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import profile\n",
    "\n",
    "# Assuming these functions are defined to work with your specific file format\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "path = \"/fsx/rs2517/data/HDTF/cropped_videos_original/WRA_VickyHartzler_000_video_512_latent.safetensors\"\n",
    "\n",
    "\n",
    "@profile\n",
    "def method1():\n",
    "    tensors = {}\n",
    "    with safe_open(path, framework=\"pt\") as f:\n",
    "        tensor_slice = f.get_slice(\"latents\")\n",
    "    print(\"Method 1: Tensor slice loaded.\")\n",
    "\n",
    "\n",
    "@profile\n",
    "def method2():\n",
    "    tensor = load_file(path)[\"latents\"]\n",
    "    print(\"Method 2: Entire tensor loaded.\")\n",
    "\n",
    "\n",
    "method1()\n",
    "method2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def logit_normal_sampler(m, s=1, beta_m=15, sample_num=1000000):\n",
    "    y_samples = torch.randn_like(m) * s + m\n",
    "    print(sample_num, s, (m + torch.randn(sample_num)).shape, torch.randn(sample_num).shape)\n",
    "    x_samples = beta_m * (torch.exp(y_samples) / (1 + torch.exp(y_samples)))\n",
    "    return x_samples\n",
    "\n",
    "\n",
    "def mu_t(t, a=5, mu_max=1):\n",
    "    t = t.to(\"cpu\")\n",
    "    return 2 * mu_max * t**a - mu_max\n",
    "\n",
    "\n",
    "def get_sigma_s(t, a, beta_m):\n",
    "    mu = mu_t(t, a=a)\n",
    "    sigma_s = logit_normal_sampler(m=mu, sample_num=t.shape[0], beta_m=beta_m)\n",
    "    return sigma_s\n",
    "\n",
    "\n",
    "sigma = torch.randn(2, 1, 1, 1)\n",
    "\n",
    "get_sigma_s(sigma, 5, 15).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected 100 video paths have been saved to /data/home/antoni/code/generative-models/tmp.txt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Path to the file containing the video paths\n",
    "file_path = \"/data/home/antoni/datasets/filelist_videos_celebv_text.txt\"\n",
    "\n",
    "\n",
    "# Function to read the file and extract video paths\n",
    "def read_video_paths(file_path):\n",
    "    video_paths = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            if \"Duration:\" in line and line.split(\".\")[0].isdigit():\n",
    "                # Extract the video path from the line\n",
    "                path = line.split(\"Path: \")[-1].strip()\n",
    "                video_paths.append(path)\n",
    "            else:\n",
    "                video_paths.append(line.strip())\n",
    "    return video_paths\n",
    "\n",
    "\n",
    "# Read all video paths\n",
    "all_video_paths = read_video_paths(file_path)\n",
    "\n",
    "# Randomly select 100 video paths\n",
    "selected_paths = random.sample(all_video_paths, min(100, len(all_video_paths)))\n",
    "\n",
    "# Save the selected paths to a new file\n",
    "output_file = \"/data/home/antoni/code/generative-models/tmp.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    for path in selected_paths:\n",
    "        file.write(f\"{path}\\n\")\n",
    "\n",
    "print(f\"Randomly selected 100 video paths have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected 500 video paths have been saved to /data/home/antoni/datasets/selected_500_video_paths_mead.txt\n",
      "First few selected paths:\n",
      "['/fsx/behavioural_computing_data/face_generation_data/MEAD/video_crop/W016/video/front/fear/level_3/018.mp4', '/fsx/behavioural_computing_data/face_generation_data/MEAD/video_crop/M022/video/front/fear/level_3/024.mp4', '/fsx/behavioural_computing_data/face_generation_data/MEAD/video_crop/M013/video/front/surprised/level_3/027.mp4', '/fsx/behavioural_computing_data/face_generation_data/MEAD/video_crop/M009/video/front/sad/level_1/005.mp4', '/fsx/behavioural_computing_data/face_generation_data/MEAD/video_crop/M005/video/front/fear/level_2/006.mp4']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Path to the file containing the video paths\n",
    "file_path = \"/data/home/antoni/datasets/filelist_video_mead.txt\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# Function to read the file and extract video paths\n",
    "def read_video_paths(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n",
    "\n",
    "# Read all video paths\n",
    "all_video_paths = read_video_paths(file_path)\n",
    "\n",
    "# Shuffle the paths\n",
    "random.shuffle(all_video_paths)\n",
    "\n",
    "# Select and verify 500 existing paths\n",
    "selected_paths = []\n",
    "for path in all_video_paths:\n",
    "    if os.path.exists(path.replace(\"video_crop\", \"video_crop_emb\").replace(\".mp4\", \"_video_512_latent.safetensors\")):\n",
    "        selected_paths.append(path)\n",
    "        if len(selected_paths) == 500:\n",
    "            break\n",
    "\n",
    "# Check if we found 500 valid paths\n",
    "if len(selected_paths) < 500:\n",
    "    print(f\"Warning: Only found {len(selected_paths)} valid paths\")\n",
    "\n",
    "# Save the selected paths to a new file\n",
    "output_file = \"/data/home/antoni/datasets/selected_500_video_paths_mead.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    for path in selected_paths:\n",
    "        file.write(f\"{path}\\n\")\n",
    "\n",
    "print(f\"Randomly selected {len(selected_paths)} video paths have been saved to {output_file}\")\n",
    "\n",
    "# Display the first few paths\n",
    "print(\"First few selected paths:\")\n",
    "print(selected_paths[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different emotions:\n",
      "fear: 73\n",
      "surprised: 72\n",
      "sad: 68\n",
      "happy: 66\n",
      "disgusted: 61\n",
      "angry: 63\n",
      "contempt: 57\n",
      "neutral: 40\n",
      "\n",
      "Number of different levels:\n",
      "level_3: 151\n",
      "level_1: 186\n",
      "level_2: 163\n",
      "\n",
      "Total number of emotions: 8\n",
      "Total number of levels: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Function to extract emotion and level from path\n",
    "def extract_emotion_and_level(path):\n",
    "    parts = path.split(\"/\")\n",
    "    emotion = parts[-3]\n",
    "    level = parts[-2]\n",
    "    return emotion, level\n",
    "\n",
    "\n",
    "# Count emotions and levels\n",
    "emotion_counter = Counter()\n",
    "level_counter = Counter()\n",
    "\n",
    "for path in selected_paths:\n",
    "    emotion, level = extract_emotion_and_level(path)\n",
    "    emotion_counter[emotion] += 1\n",
    "    level_counter[level] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Number of different emotions:\")\n",
    "for emotion, count in emotion_counter.items():\n",
    "    print(f\"{emotion}: {count}\")\n",
    "\n",
    "print(\"\\nNumber of different levels:\")\n",
    "for level, count in level_counter.items():\n",
    "    print(f\"{level}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal number of emotions: {len(emotion_counter)}\")\n",
    "print(f\"Total number of levels: {len(level_counter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None, 0, 13, 26, 39, 52, 65, 78, 91, 104, 117, 130, 143, 156], [None, 169, 182, 195, 208, 221, 234, 247, 260, 273, 286, 299, 312, 325], [None, 338, 351, 364, 377, 390, 403, 416, 429, 442, 455, 468, 481, 494], [None, 507, 520, 533, 546, 559, 572, 585, 598, 611, 624, 637, 650, 663], [None, 676, 689, 702, 715, 728, 741, 754, 767, 780, 793, 806, 819, 832], [None, 845, 858, 871, 884, 897, 910, 923, 936, 949, 962, 975, 988, 1001], [None, 1014, 1027, 1040, 1053, 1066, 1079]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 494\n",
      "[0, 13] [481, 494]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "494 1001\n",
      "[494, 507] [988, 1001]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1001 1079\n",
      "[1001, 1014] [1066, 1079]\n"
     ]
    }
   ],
   "source": [
    "keyframes = [\n",
    "    None,\n",
    "    0,\n",
    "    13,\n",
    "    26,\n",
    "    39,\n",
    "    52,\n",
    "    65,\n",
    "    78,\n",
    "    91,\n",
    "    104,\n",
    "    117,\n",
    "    130,\n",
    "    143,\n",
    "    156,\n",
    "    None,\n",
    "    169,\n",
    "    182,\n",
    "    195,\n",
    "    208,\n",
    "    221,\n",
    "    234,\n",
    "    247,\n",
    "    260,\n",
    "    273,\n",
    "    286,\n",
    "    299,\n",
    "    312,\n",
    "    325,\n",
    "    None,\n",
    "    338,\n",
    "    351,\n",
    "    364,\n",
    "    377,\n",
    "    390,\n",
    "    403,\n",
    "    416,\n",
    "    429,\n",
    "    442,\n",
    "    455,\n",
    "    468,\n",
    "    481,\n",
    "    494,\n",
    "    None,\n",
    "    507,\n",
    "    520,\n",
    "    533,\n",
    "    546,\n",
    "    559,\n",
    "    572,\n",
    "    585,\n",
    "    598,\n",
    "    611,\n",
    "    624,\n",
    "    637,\n",
    "    650,\n",
    "    663,\n",
    "    None,\n",
    "    676,\n",
    "    689,\n",
    "    702,\n",
    "    715,\n",
    "    728,\n",
    "    741,\n",
    "    754,\n",
    "    767,\n",
    "    780,\n",
    "    793,\n",
    "    806,\n",
    "    819,\n",
    "    832,\n",
    "    None,\n",
    "    845,\n",
    "    858,\n",
    "    871,\n",
    "    884,\n",
    "    897,\n",
    "    910,\n",
    "    923,\n",
    "    936,\n",
    "    949,\n",
    "    962,\n",
    "    975,\n",
    "    988,\n",
    "    1001,\n",
    "    None,\n",
    "    1014,\n",
    "    1027,\n",
    "    1040,\n",
    "    1053,\n",
    "    1066,\n",
    "    1079,\n",
    "]\n",
    "\n",
    "combinations = [\n",
    "    [0, 13],\n",
    "    [13, 26],\n",
    "    [26, 39],\n",
    "    [39, 52],\n",
    "    [52, 65],\n",
    "    [65, 78],\n",
    "    [78, 91],\n",
    "    [91, 104],\n",
    "    [104, 117],\n",
    "    [117, 130],\n",
    "    [130, 143],\n",
    "    [143, 156],\n",
    "    [156, 169],\n",
    "    [169, 182],\n",
    "    [182, 195],\n",
    "    [195, 208],\n",
    "    [208, 221],\n",
    "    [221, 234],\n",
    "    [234, 247],\n",
    "    [247, 260],\n",
    "    [260, 273],\n",
    "    [273, 286],\n",
    "    [286, 299],\n",
    "    [299, 312],\n",
    "    [312, 325],\n",
    "    [325, 338],\n",
    "    [338, 351],\n",
    "    [351, 364],\n",
    "    [364, 377],\n",
    "    [377, 390],\n",
    "    [390, 403],\n",
    "    [403, 416],\n",
    "    [416, 429],\n",
    "    [429, 442],\n",
    "    [442, 455],\n",
    "    [455, 468],\n",
    "    [468, 481],\n",
    "    [481, 494],\n",
    "    [494, 507],\n",
    "    [507, 520],\n",
    "    [520, 533],\n",
    "    [533, 546],\n",
    "    [546, 559],\n",
    "    [559, 572],\n",
    "    [572, 585],\n",
    "    [585, 598],\n",
    "    [598, 611],\n",
    "    [611, 624],\n",
    "    [624, 637],\n",
    "    [637, 650],\n",
    "    [650, 663],\n",
    "    [663, 676],\n",
    "    [676, 689],\n",
    "    [689, 702],\n",
    "    [702, 715],\n",
    "    [715, 728],\n",
    "    [728, 741],\n",
    "    [741, 754],\n",
    "    [754, 767],\n",
    "    [767, 780],\n",
    "    [780, 793],\n",
    "    [793, 806],\n",
    "    [806, 819],\n",
    "    [819, 832],\n",
    "    [832, 845],\n",
    "    [845, 858],\n",
    "    [858, 871],\n",
    "    [871, 884],\n",
    "    [884, 897],\n",
    "    [897, 910],\n",
    "    [910, 923],\n",
    "    [923, 936],\n",
    "    [936, 949],\n",
    "    [949, 962],\n",
    "    [962, 975],\n",
    "    [975, 988],\n",
    "    [988, 1001],\n",
    "    [1001, 1014],\n",
    "    [1014, 1027],\n",
    "    [1027, 1040],\n",
    "    [1040, 1053],\n",
    "    [1053, 1066],\n",
    "    [1066, 1079],\n",
    "]\n",
    "\n",
    "# Define the chunk size as a multiple of 14\n",
    "chunk_size = 14  # You can change this to any multiple of 14\n",
    "num_frames = 14\n",
    "\n",
    "# Ensure the chunk size is a multiple of 14\n",
    "if chunk_size % 14 != 0:\n",
    "    raise ValueError(\"Chunk size must be a multiple of 14\")\n",
    "\n",
    "keyframes_list = [keyframes[i : i + num_frames] for i in range(0, len(keyframes), num_frames)]\n",
    "print(keyframes_list)\n",
    "chunk_size = 3\n",
    "last_keyframe_prev = None\n",
    "for i in range(0, len(keyframes_list), chunk_size):\n",
    "    start = i\n",
    "    end = i + chunk_size\n",
    "\n",
    "    unflattened_keyframes = [elem for sublist in keyframes_list[start:end] for elem in sublist]\n",
    "\n",
    "    if last_keyframe_prev is not None:\n",
    "        unflattened_keyframes = [last_keyframe_prev] + unflattened_keyframes\n",
    "\n",
    "    # Find the first non-None keyframe in the chunk\n",
    "    first_keyframe = next((kf for kf in unflattened_keyframes if kf is not None), None)\n",
    "\n",
    "    # Find the last non-None keyframe in the chunk\n",
    "    last_keyframe = next((kf for kf in reversed(unflattened_keyframes) if kf is not None), None)\n",
    "\n",
    "    # Store the last keyframe for the next iteration\n",
    "    last_keyframe_prev = last_keyframe\n",
    "\n",
    "    # Find the corresponding combinations and their indices\n",
    "    start_idx = next((idx for idx, comb in enumerate(combinations) if comb[0] == first_keyframe), None)\n",
    "    end_idx = next((idx for idx, comb in enumerate(reversed(combinations)) if comb[1] == last_keyframe), None)\n",
    "\n",
    "    if start_idx is not None and end_idx is not None:\n",
    "        end_idx = len(combinations) - 1 - end_idx  # Adjust for reversed enumeration\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(first_keyframe, last_keyframe)\n",
    "    print(combinations[start_idx : end_idx + 1][0], combinations[start_idx : end_idx + 1][-1])\n",
    "\n",
    "    # If we've reached the end of the keyframes list, break the loop\n",
    "    # if end >= len(keyframes_list):\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination from keyframes: 0 -> 13 corresponds to interval: [0, 13]\n",
      "Combination from keyframes: 13 -> 26 corresponds to interval: [13, 26]\n",
      "Combination from keyframes: 26 -> 39 corresponds to interval: [26, 39]\n",
      "Combination from keyframes: 39 -> 52 corresponds to interval: [39, 52]\n",
      "Combination from keyframes: 52 -> 65 corresponds to interval: [52, 65]\n",
      "Combination from keyframes: 65 -> 78 corresponds to interval: [65, 78]\n",
      "Combination from keyframes: 78 -> 91 corresponds to interval: [78, 91]\n",
      "Combination from keyframes: 91 -> 104 corresponds to interval: [91, 104]\n",
      "Combination from keyframes: 104 -> 117 corresponds to interval: [104, 117]\n",
      "Combination from keyframes: 117 -> 130 corresponds to interval: [117, 130]\n",
      "Combination from keyframes: 130 -> 143 corresponds to interval: [130, 143]\n",
      "Combination from keyframes: 143 -> 156 corresponds to interval: [143, 156]\n",
      "Combination from keyframes: 156 -> 169 corresponds to interval: [156, 169]\n",
      "Combination from keyframes: 169 -> 182 corresponds to interval: [169, 182]\n",
      "Combination from keyframes: 182 -> 195 corresponds to interval: [182, 195]\n",
      "Combination from keyframes: 195 -> 208 corresponds to interval: [195, 208]\n",
      "Combination from keyframes: 208 -> 221 corresponds to interval: [208, 221]\n",
      "Combination from keyframes: 221 -> 234 corresponds to interval: [221, 234]\n",
      "Combination from keyframes: 234 -> 247 corresponds to interval: [234, 247]\n",
      "Combination from keyframes: 247 -> 260 corresponds to interval: [247, 260]\n",
      "Combination from keyframes: 260 -> 273 corresponds to interval: [260, 273]\n",
      "Combination from keyframes: 273 -> 286 corresponds to interval: [273, 286]\n",
      "Combination from keyframes: 286 -> 299 corresponds to interval: [286, 299]\n",
      "Combination from keyframes: 299 -> 312 corresponds to interval: [299, 312]\n",
      "Combination from keyframes: 312 -> 325 corresponds to interval: [312, 325]\n",
      "Combination from keyframes: 338 -> 351 corresponds to interval: [26, 39]\n",
      "Combination from keyframes: 351 -> 364 corresponds to interval: [39, 52]\n",
      "Combination from keyframes: 364 -> 377 corresponds to interval: [52, 65]\n",
      "Combination from keyframes: 377 -> 390 corresponds to interval: [65, 78]\n",
      "Combination from keyframes: 390 -> 403 corresponds to interval: [78, 91]\n",
      "Combination from keyframes: 403 -> 416 corresponds to interval: [91, 104]\n",
      "Combination from keyframes: 416 -> 429 corresponds to interval: [104, 117]\n",
      "Combination from keyframes: 429 -> 442 corresponds to interval: [117, 130]\n",
      "Combination from keyframes: 442 -> 455 corresponds to interval: [130, 143]\n",
      "Combination from keyframes: 455 -> 468 corresponds to interval: [143, 156]\n",
      "Combination from keyframes: 468 -> 481 corresponds to interval: [156, 169]\n",
      "Combination from keyframes: 481 -> 494 corresponds to interval: [169, 182]\n",
      "Combination from keyframes: 494 -> 507 corresponds to interval: [182, 195]\n",
      "Combination from keyframes: 507 -> 520 corresponds to interval: [195, 208]\n",
      "Combination from keyframes: 520 -> 533 corresponds to interval: [208, 221]\n",
      "Combination from keyframes: 533 -> 546 corresponds to interval: [221, 234]\n",
      "Combination from keyframes: 546 -> 559 corresponds to interval: [234, 247]\n",
      "Combination from keyframes: 559 -> 572 corresponds to interval: [247, 260]\n",
      "Combination from keyframes: 572 -> 585 corresponds to interval: [260, 273]\n",
      "Combination from keyframes: 585 -> 598 corresponds to interval: [273, 286]\n",
      "Combination from keyframes: 598 -> 611 corresponds to interval: [286, 299]\n",
      "Combination from keyframes: 611 -> 624 corresponds to interval: [299, 312]\n",
      "Combination from keyframes: 624 -> 637 corresponds to interval: [312, 325]\n",
      "Combination from keyframes: 637 -> 650 corresponds to interval: [325, 338]\n",
      "Combination from keyframes: 650 -> 663 corresponds to interval: [338, 351]\n",
      "Combination from keyframes: 676 -> 689 corresponds to interval: [52, 65]\n",
      "Combination from keyframes: 689 -> 702 corresponds to interval: [65, 78]\n",
      "Combination from keyframes: 702 -> 715 corresponds to interval: [78, 91]\n",
      "Combination from keyframes: 715 -> 728 corresponds to interval: [91, 104]\n",
      "Combination from keyframes: 728 -> 741 corresponds to interval: [104, 117]\n",
      "Combination from keyframes: 741 -> 754 corresponds to interval: [117, 130]\n",
      "Combination from keyframes: 754 -> 767 corresponds to interval: [130, 143]\n",
      "Combination from keyframes: 767 -> 780 corresponds to interval: [143, 156]\n",
      "Combination from keyframes: 780 -> 793 corresponds to interval: [156, 169]\n",
      "Combination from keyframes: 793 -> 806 corresponds to interval: [169, 182]\n",
      "Combination from keyframes: 806 -> 819 corresponds to interval: [182, 195]\n",
      "Combination from keyframes: 819 -> 832 corresponds to interval: [195, 208]\n",
      "Combination from keyframes: 832 -> 845 corresponds to interval: [208, 221]\n",
      "Combination from keyframes: 845 -> 858 corresponds to interval: [221, 234]\n",
      "Combination from keyframes: 858 -> 871 corresponds to interval: [234, 247]\n",
      "Combination from keyframes: 871 -> 884 corresponds to interval: [247, 260]\n",
      "Combination from keyframes: 884 -> 897 corresponds to interval: [260, 273]\n",
      "Combination from keyframes: 897 -> 910 corresponds to interval: [273, 286]\n",
      "Combination from keyframes: 910 -> 923 corresponds to interval: [286, 299]\n",
      "Combination from keyframes: 923 -> 936 corresponds to interval: [299, 312]\n",
      "Combination from keyframes: 936 -> 949 corresponds to interval: [312, 325]\n",
      "Combination from keyframes: 949 -> 962 corresponds to interval: [325, 338]\n",
      "Combination from keyframes: 962 -> 975 corresponds to interval: [338, 351]\n",
      "Combination from keyframes: 975 -> 988 corresponds to interval: [351, 364]\n",
      "Combination from keyframes: 988 -> 1001 corresponds to interval: [364, 377]\n",
      "Combination from keyframes: 1014 -> 1027 corresponds to interval: [78, 91]\n",
      "Combination from keyframes: 1027 -> 1040 corresponds to interval: [91, 104]\n",
      "Combination from keyframes: 1040 -> 1053 corresponds to interval: [104, 117]\n",
      "Combination from keyframes: 1053 -> 1066 corresponds to interval: [117, 130]\n",
      "Combination from keyframes: 1066 -> 1079 corresponds to interval: [130, 143]\n"
     ]
    }
   ],
   "source": [
    "keyframes_chunks = [\n",
    "    [None, 0, 13, 26, 39, 52, 65, 78, 91, 104, 117, 130, 143, 156],\n",
    "    [None, 169, 182, 195, 208, 221, 234, 247, 260, 273, 286, 299, 312, 325],\n",
    "    [None, 338, 351, 364, 377, 390, 403, 416, 429, 442, 455, 468, 481, 494],\n",
    "    [None, 507, 520, 533, 546, 559, 572, 585, 598, 611, 624, 637, 650, 663],\n",
    "    [None, 676, 689, 702, 715, 728, 741, 754, 767, 780, 793, 806, 819, 832],\n",
    "    [None, 845, 858, 871, 884, 897, 910, 923, 936, 949, 962, 975, 988, 1001],\n",
    "    [None, 1014, 1027, 1040, 1053, 1066, 1079],\n",
    "]\n",
    "\n",
    "combinations = [\n",
    "    [0, 13],\n",
    "    [13, 26],\n",
    "    [26, 39],\n",
    "    [39, 52],\n",
    "    [52, 65],\n",
    "    [65, 78],\n",
    "    [78, 91],\n",
    "    [91, 104],\n",
    "    [104, 117],\n",
    "    [117, 130],\n",
    "    [130, 143],\n",
    "    [143, 156],\n",
    "    [156, 169],\n",
    "    [169, 182],\n",
    "    [182, 195],\n",
    "    [195, 208],\n",
    "    [208, 221],\n",
    "    [221, 234],\n",
    "    [234, 247],\n",
    "    [247, 260],\n",
    "    [260, 273],\n",
    "    [273, 286],\n",
    "    [286, 299],\n",
    "    [299, 312],\n",
    "    [312, 325],\n",
    "    [325, 338],\n",
    "    [338, 351],\n",
    "    [351, 364],\n",
    "    [364, 377],\n",
    "    [377, 390],\n",
    "    [390, 403],\n",
    "    [403, 416],\n",
    "    [416, 429],\n",
    "    [429, 442],\n",
    "    [442, 455],\n",
    "    [455, 468],\n",
    "    [468, 481],\n",
    "    [481, 494],\n",
    "    [494, 507],\n",
    "    [507, 520],\n",
    "    [520, 533],\n",
    "    [533, 546],\n",
    "    [546, 559],\n",
    "    [559, 572],\n",
    "    [572, 585],\n",
    "    [585, 598],\n",
    "    [598, 611],\n",
    "    [611, 624],\n",
    "    [624, 637],\n",
    "    [637, 650],\n",
    "    [650, 663],\n",
    "    [663, 676],\n",
    "    [676, 689],\n",
    "    [689, 702],\n",
    "    [702, 715],\n",
    "    [715, 728],\n",
    "    [728, 741],\n",
    "    [741, 754],\n",
    "    [754, 767],\n",
    "    [767, 780],\n",
    "    [780, 793],\n",
    "    [793, 806],\n",
    "    [806, 819],\n",
    "    [819, 832],\n",
    "    [832, 845],\n",
    "    [845, 858],\n",
    "    [858, 871],\n",
    "    [871, 884],\n",
    "    [884, 897],\n",
    "    [897, 910],\n",
    "    [910, 923],\n",
    "    [923, 936],\n",
    "    [936, 949],\n",
    "    [949, 962],\n",
    "    [962, 975],\n",
    "    [975, 988],\n",
    "    [988, 1001],\n",
    "    [1001, 1014],\n",
    "    [1014, 1027],\n",
    "    [1027, 1040],\n",
    "    [1040, 1053],\n",
    "    [1053, 1066],\n",
    "    [1066, 1079],\n",
    "]\n",
    "\n",
    "# Define the chunk size you want to process at once (must be a multiple of 14)\n",
    "chunk_size = 28  # This example processes two chunks at a time (2 * 14 = 28)\n",
    "\n",
    "# Ensure chunk size is a multiple of 14\n",
    "if chunk_size % 14 != 0:\n",
    "    raise ValueError(\"Chunk size must be a multiple of 14\")\n",
    "\n",
    "# Flatten the list of keyframes (since chunks are in separate lists)\n",
    "flat_keyframes = [kf for chunk in keyframes_chunks for kf in chunk]\n",
    "\n",
    "# Iterate through the flattened keyframes in blocks of 'chunk_size'\n",
    "for i in range(0, len(flat_keyframes), chunk_size):\n",
    "    keyframe_chunk = flat_keyframes[i : i + chunk_size]  # Get a chunk of the flattened keyframes\n",
    "    valid_keyframes = [kf for kf in keyframe_chunk if kf is not None]  # Skip 'None'\n",
    "\n",
    "    # Form pairs and match them with combinations\n",
    "    for j in range(len(valid_keyframes) - 1):\n",
    "        start_idx = valid_keyframes[j]\n",
    "        end_idx = valid_keyframes[j + 1]\n",
    "\n",
    "        # Calculate the combination index and ensure it's within bounds\n",
    "        combination_idx = i // 14 + j\n",
    "        if combination_idx < len(combinations):\n",
    "            combination = combinations[combination_idx]\n",
    "            print(f\"Combination from keyframes: {start_idx} -> {end_idx} corresponds to interval: {combination}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
